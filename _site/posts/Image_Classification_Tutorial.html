<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Walter Wiggins">
<meta name="dcterms.date" content="2023-11-30">

<title>Radiology ML Tutorials - Image Classification for Beginners</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Radiology ML Tutorials</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/wfwiggins" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/walterfwiggins" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Image Classification for Beginners</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">code</div>
                <div class="quarto-category">imaging</div>
                <div class="quarto-category">classification</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Walter Wiggins </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">2023-11-30</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p><a href="https://colab.research.google.com/github/wfwiggins/rad-ml-tutor/blob/master/Image_Classification_Tutorial.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/wfwiggins/RSNA-Image-AI-2020/blob/master/images/RSNA-2020-logo-with-dates.gif?raw=true" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">RSNA logo</figcaption>
</figure>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In this demonstration, we will utilize techniques of <em>computer vision</em>, including deep <em>convolutional neural networks</em> (CNNs), to train an image classifier model capable of classifying radiographs as either <strong>chest</strong> or <strong>abdominal</strong>.</p>
<section id="code" class="level3">
<h3 class="anchored" data-anchor-id="code">Code</h3>
<p>We will utilize the <a href="https://docs.fast.ai/">fast.ai v2 library</a>, written primarily by Jeremy Howard and Sylvain Gugger (with help from many others). It is written in the <a href="https://www.python.org/">Python programming language</a> and built on top of the <a href="https://www.pytorch.org/">PyTorch deep learning library</a>.</p>
<p>The demonstration in this notebook relies heavily on examples from the <code>fast.ai</code> book, <em>Deep Learning for Coders with fastai and PyTorch: AI Applications without a PhD</em> by Jeremy Howard and Sylvain Gugger, which was written entirely in Jupyter notebooks, which are <a href="https://github.com/fastai/fastbook">freely available for download on GitHub</a>. A print copy of the book can be purchased from Amazon.</p>
</section>
<section id="data" class="level3">
<h3 class="anchored" data-anchor-id="data">Data</h3>
<p>This work is adapted from ‚Äú<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5959832/">Hello World Deep Learning in Medical Imaging</a> {% fn 1 %}‚Äù. The chest and abdominal radiographs were obtained from <a href="https://github.com/paras42/Hello_World_Deep_Learning/tree/9921a12c905c00a88898121d5dc538e3b524e520">Paras Lakhani‚Äôs GitHub repository</a>.</p>
<p>{{ ‚Äú<em>Reference:</em> Lakhani P, Gray DL, Pett CR, Nagy P, Shih G. Hello World Deep Learning in Medical Imaging. J Digit Imaging. 2018 Jun; 31(3):283-289. Published online 2018 May 3. doi: 10.1007/s10278-018-0779-6‚Äù | fndetail: 1 }}</p>
</section>
<section id="developers" class="level3">
<h3 class="anchored" data-anchor-id="developers">Developers</h3>
<ul>
<li>Walter F. Wiggins, MD, PhD - Duke University Hospital, Durham, NC, USA</li>
<li>Kirti Magudia, MD, PhD, - University of California, San Francisco, CA, USA</li>
<li>M. Travis Caton, MD, PhD - University of California, San Francisco, CA, USA</li>
</ul>
</section>
<section id="acknowledgements" class="level3">
<h3 class="anchored" data-anchor-id="acknowledgements">Acknowledgements</h3>
<p>Other versions of this notebook implemented on the <a href="https://www.kaggle.com/wfwiggins203/hello-world-for-deep-learning-siim">Kaggle Notebooks platform</a> were presented at the 2019 Society for Imaging Informatics in Medicine (SIIM) Annual Meeting and for the American College of Radiology (ACR) Residents &amp; Fellows Section (RFS) <a href="https://www.acr.org/Member-Resources/rfs/Journal-Club">AI Journal Club</a>.</p>
<p>We would also like to acknowledge the following individuals for inspiring our transition to the Google Colab platform with their excellent notebook from the 2019 RSNA AI Refresher Course: - Luciano M. Prevedello, MD, PhD - Felipe C. Kitamura, MD, MSc - Igor Santos, MD - Ian Pan, MD</p>
</section>
</section>
<section id="system-setup-downloading-the-data" class="level2">
<h2 class="anchored" data-anchor-id="system-setup-downloading-the-data">System Setup &amp; Downloading the Data</h2>
<blockquote class="blockquote">
<p>Important: Save a copy of this notebook in your Google Drive folder by selecting <em>Save a Copy in Drive</em> from the <em>File</em> menu in the top left corner of this page. This will allow you to modify the cells and save your results.</p>
</blockquote>
<p><br></p>
<blockquote class="blockquote">
<p>Warning: Make sure you have the <em>runtime type</em> set to <strong>‚ÄúGPU‚Äù</strong>. See GIF below.</p>
</blockquote>
<p><br></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/wfwiggins/RSNA-Image-AI-2020/blob/master/images/set-runtime-type.gif?raw=true" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Set Runtime to GPU</figcaption>
</figure>
</div>
<section id="setting-up-the-runtime-environment" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-the-runtime-environment">Setting up the runtime environment‚Ä¶</h3>
<p>Running the following cell in Colab will install the necessary libraries, download the data and restart the session.</p>
<blockquote class="blockquote">
<p>Warning: This will generate an error message, which we can safely ignore üòâ.</p>
</blockquote>
<p>::: {.cell _uuid=‚Äò0c1e51dbcbe223c29555c5188b0df55b10ed8b06‚Äô cellView=‚Äòform‚Äô}</p>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install fastai<span class="op">==</span><span class="fl">2.1.4</span> <span class="op">&gt;/</span>dev<span class="op">/</span>null</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install fastcore<span class="op">==</span><span class="fl">1.3.1</span> <span class="op">&gt;/</span>dev<span class="op">/</span>null</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># **Downloading the data...**</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>wget <span class="op">-</span>q https:<span class="op">//</span>github.com<span class="op">/</span>wfwiggins<span class="op">/</span>RSNA<span class="op">-</span>Image<span class="op">-</span>AI<span class="op">-</span><span class="dv">2020</span><span class="op">/</span>blob<span class="op">/</span>master<span class="op">/</span>data.<span class="bu">zip</span>?raw<span class="op">=</span>true</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir <span class="op">-</span>p data</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>unzip <span class="op">-</span>o data.<span class="bu">zip</span>?raw<span class="op">=</span>true <span class="op">-</span>d data <span class="op">&gt;/</span>dev<span class="op">/</span>null</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>rm data.<span class="bu">zip</span>?raw<span class="op">=</span>true</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>os.kill(os.getpid(), <span class="dv">9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>:::</p>
</section>
</section>
<section id="exploring-the-data" class="level2">
<h2 class="anchored" data-anchor-id="exploring-the-data">Exploring the Data</h2>
<p>Let‚Äôs take a look at the directory structure and contents, then create some variables to help us as we proceed.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">#collapse</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>warnings.simplefilter(<span class="st">'ignore'</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> (<span class="dv">12</span>, <span class="dv">12</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>::: {.cell _uuid=‚Äò4cfcb87c72a542aefce13f8453097c0d1eb0c7b9‚Äô cellView=‚Äòform‚Äô outputId=‚Äòc18af866-27cb-4f1b-b4a9-7e76ce8456e2‚Äô execution_count=2}</p>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.basics <span class="im">import</span> <span class="op">*</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.vision.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set path variable to the directory where the data is located</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> Path(<span class="st">'/content/data'</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Command line "magic" command to show directory contents</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>ls {path}<span class="op">/**/*</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>/content/data/test/abd:
abd_test.png

/content/data/test/chest:
chest_test.png

/content/data/train/abd:
abd0.png   abd14.png  abd19.png  abd23.png  abd28.png  abd3.png  abd8.png
abd10.png  abd15.png  abd1.png   abd24.png  abd29.png  abd4.png  abd9.png
abd11.png  abd16.png  abd20.png  abd25.png  abd2.png   abd5.png
abd12.png  abd17.png  abd21.png  abd26.png  abd30.png  abd6.png
abd13.png  abd18.png  abd22.png  abd27.png  abd31.png  abd7.png

/content/data/train/chest:
chst33.png  chst39.png  chst45.png  chst51.png  chst57.png  chst63.png
chst34.png  chst40.png  chst46.png  chst52.png  chst58.png  chst64.png
chst35.png  chst41.png  chst47.png  chst53.png  chst59.png  chst65.png
chst36.png  chst42.png  chst48.png  chst54.png  chst60.png
chst37.png  chst43.png  chst49.png  chst55.png  chst61.png
chst38.png  chst44.png  chst50.png  chst56.png  chst62.png

/content/data/val/abd:
abd0.png  abd1.png  abd2.png  abd3.png  abd4.png

/content/data/val/chest:
chst0.png  chst1.png  chst2.png  chst3.png  chst4.png</code></pre>
</div>
<p>:::</p>
<p>As you can see, the <code>data</code> directory contains subdirectories <code>train</code>, <code>val</code> and <code>test</code>, which contain the <em>training</em>, <em>validation</em> and <em>test</em> data for our experiment. <code>train</code> and <code>val</code> contain subdirectories <code>abd</code> and <code>chest</code> containing abdominal and chest radiographs for each data set. There are 65 training images and 10 validation images with <em>balanced distributions</em> over our <em>target classes</em> (i.e.&nbsp;approximately equal numbers of abdominal and chest radiographs in each data set and optimized for a classification problem).</p>
</section>
<section id="model-training-setup" class="level2">
<h2 class="anchored" data-anchor-id="model-training-setup">Model Training Setup</h2>
<p>Before we train the model, we have to get the data in a format such that it can be presented to the model for training.</p>
<section id="data-loaders" class="level3">
<h3 class="anchored" data-anchor-id="data-loaders">Data Loaders</h3>
<p>The first step is to load the data for the training and validation datasets into a <code>ImageDataLoaders</code> object from the <code>fastai</code> library. When training a model, the <code>ImageDataLoaders</code> will present training - and subsequently, validation - data to the model in <em>batches</em>.</p>
</section>
<section id="data-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="data-augmentation">Data Augmentation</h3>
<p>In order to be sure that the model isn‚Äôt simply ‚Äúmemorizing‚Äù the training data, we will <em>augment</em> the data by randomly applying different <em>transformations</em> to each image before it is sent to the model.</p>
<p>Transformations can include rotation, translation, flipping, rescaling, etc.</p>
</section>
<section id="load-the-data-into-imagedataloaders-with-data-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="load-the-data-into-imagedataloaders-with-data-augmentation">Load the data into <code>ImageDataLoaders</code> with data augmentation</h3>
<blockquote class="blockquote">
<p>Note: When you run this next cell in Colab, a batch of data will be shown with or without augmentation transforms applied. (1) Run this cell once with the box next to <code>apply_transforms</code> unchecked to see a sample of the original images. (2) Next, run the cell a few more times after checking the box next to <code>apply_transforms</code> to see what happens to the images when the transforms are applied.</p>
</blockquote>
<p>::: {.cell _uuid=‚Äòd1c24e4a78f57f12a42de3481db746fe0f170ee3‚Äô cellView=‚Äòform‚Äô outputId=‚Äòba44aa2d-d8f1-40b2-e9ef-64500483f516‚Äô execution_count=3}</p>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># the following line of code utilizes Colab Forms</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>apply_transforms <span class="op">=</span> <span class="va">True</span> <span class="co">#@param {type: 'boolean'}</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> apply_transforms:</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    flip <span class="op">=</span> <span class="va">True</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    max_rotate <span class="op">=</span> <span class="fl">10.0</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    max_warp <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    p_affine <span class="op">=</span> <span class="fl">0.75</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    flip <span class="op">=</span> <span class="va">False</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    max_rotate, max_warp, p_affine <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>tfms <span class="op">=</span> aug_transforms(</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    do_flip<span class="op">=</span>flip,</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    max_rotate<span class="op">=</span>max_rotate,</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    max_warp<span class="op">=</span>max_warp,</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    p_affine<span class="op">=</span>p_affine,</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    size<span class="op">=</span><span class="dv">224</span>,</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    min_scale<span class="op">=</span><span class="fl">0.75</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> ImageDataLoaders.from_folder(path, valid<span class="op">=</span><span class="st">'val'</span>, seed<span class="op">=</span><span class="dv">42</span>, item_tfms<span class="op">=</span>Resize(<span class="dv">460</span>), batch_tfms<span class="op">=</span>tfms, bs<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>dls.show_batch(max_n<span class="op">=</span><span class="dv">6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Image_Classification_Tutorial_files/figure-html/cell-5-output-1.png" class="img-fluid"></p>
</div>
<p>:::</p>
</section>
<section id="find-the-optimal-learning-rate" class="level3">
<h3 class="anchored" data-anchor-id="find-the-optimal-learning-rate">Find the optimal learning rate</h3>
<p>The learning rate is a hyperparameter that controls how much your model adjusts in response to percieved error after each training epoch. Choosing an optimal learning rate is an optimal step in model training.</p>
<p>From the <code>fastai</code> <a href="https://docs.fast.ai/callback.schedule#Learner.lr_find">docs</a>: &gt; First introduced by Leslie N. Smith in <a href="https://arxiv.org/pdf/1506.01186.pdf">Cyclical Learning Rates for Training Neural Networks</a>, the <code>LRFinder</code> trains the model with exponentially growing learning rates and stops in case of divergence. &gt; The losses are then plotted against the learning rates with a log scale. <br><br> &gt; A good value for the learning rates is then either: &gt; - 1/10th of the minimum before the divergence &gt; - where the slope is the steepest</p>
<p><br></p>
<blockquote class="blockquote">
<p>Note: When you run this cell for the first time in a Colab session, it will download a pretrained version of the model to your workspace before running the <code>LRFinder</code>.</p>
</blockquote>
<div class="cell" data-cellview="form" data-outputid="ac4ee915-029c-43a0-a02a-0bdc5a41d5e9" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> ImageDataLoaders.from_folder(path, valid<span class="op">=</span><span class="st">'val'</span>, seed<span class="op">=</span><span class="dv">42</span>, item_tfms<span class="op">=</span>Resize(<span class="dv">460</span>), batch_tfms<span class="op">=</span>aug_transforms(size<span class="op">=</span><span class="dv">224</span>, min_scale<span class="op">=</span><span class="fl">0.75</span>), bs<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> cnn_learner(dls, resnet18, metrics<span class="op">=</span>accuracy)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>learn.lr_find()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Downloading: "https://download.pytorch.org/models/resnet18-5c106cde.pth" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"42e792826caa4935b7adbf5432268259","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<p><img src="Image_Classification_Tutorial_files/figure-html/cell-6-output-5.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="transfer-learning" class="level2">
<h2 class="anchored" data-anchor-id="transfer-learning">Transfer Learning</h2>
<p>Deep learning requires large amounts of training data to successfully train a model.</p>
<p>When we don‚Äôt have enough data to work with for the planned task, starting with a <em>pre-trained</em> network that has been optimally trained on another task can be helpful. The concept of re-training a pre-trained network for a different task is called <em>transfer learning</em>.</p>
<section id="fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning">Fine-tuning</h3>
<p>In the process of re-training the model, we start by changing the final layers of the network to define the output or predictions our model will make. In order to avoid propagating too much error through the rest of the network during the initial training, we freeze the other layers of the network for the first cycle or <em>epoch</em> of training. Next, we open up the rest of the network for training and train for a few more <em>epochs</em>. This process is called <em>fine-tuning</em>.</p>
</section>
<section id="epochs-and-data-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="epochs-and-data-augmentation">Epochs and data augmentation</h3>
<p>During each epoch, the model will be exposed to the entire dataset. Each batch of data will have our data transformations randomly applied in order to provide data augmentation. This helps to ensure that our model never sees the exact same image twice. This is important because we wouldn‚Äôt want our model to simply memorize the training dataset and not converge on a generalized solution, resulting in poor performance on the validation dataset.</p>
</section>
<section id="the-loss-function" class="level3">
<h3 class="anchored" data-anchor-id="the-loss-function">The loss function</h3>
<p>In a classification task, you‚Äôre either right or wrong. This binary information doesn‚Äôt give us much nuance to work with when training a model. A <em>loss function</em> give us a numeric estimation of ‚Äúhow wrong‚Äù our model is. This gives us a target to optimize during the training process.</p>
<p>When reviewing the results of successive epochs in training, the loss on your validation dataset should always be <strong>decreasing</strong>. When it starts to increase, that is a sign of your model <em>overfitting</em> to the training dataset.</p>
</section>
<section id="fine-tuning-the-model" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-the-model">Fine-tuning the model</h3>
<p>We will fine-tune our model to our task in the following steps: 1. Select the number of epochs for which we will train the model 2. Choose a base learning rate based on the results from the <code>LRFinder</code> plot above 3. Run the cell to initiate model training utilizing the <code>fine_tune()</code> method from <code>fastai</code></p>
<blockquote class="blockquote">
<p>Tip: If you‚Äôre running this notebook in Colab, you can re-run this cell with different hyperparameters to better understand how they affect the result.</p>
</blockquote>
<div class="cell" data-cellview="form" data-outputid="89822ba0-4618-49a4-fd53-51080a6e35b6" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># the following lines of code utilize Colab Forms</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span> <span class="co">#@param {type: "integer"}</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>base_lr <span class="op">=</span> <span class="fl">2e-3</span> <span class="co">#@param {type: "number"}</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> cnn_learner(dls, resnet18, metrics<span class="op">=</span>accuracy)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>learn.fine_tune(epochs, base_lr<span class="op">=</span>base_lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.106555</td>
<td>1.336308</td>
<td>0.500000</td>
<td>00:01</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.124234</td>
<td>0.632943</td>
<td>0.700000</td>
<td>00:01</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.175012</td>
<td>0.107626</td>
<td>0.900000</td>
<td>00:01</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.129196</td>
<td>0.022088</td>
<td>1.000000</td>
<td>00:01</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.100126</td>
<td>0.012920</td>
<td>1.000000</td>
<td>00:01</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.078639</td>
<td>0.012502</td>
<td>1.000000</td>
<td>00:01</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="review-training-curves" class="level3">
<h3 class="anchored" data-anchor-id="review-training-curves">Review training curves</h3>
<p>The visual representation of the training and validation losses are useful to evaluate how successfully you were able to train your model. You should see the validation loss continuously decreasing over subsequent batches.</p>
<blockquote class="blockquote">
<p>Important: If the validation loss begins to increase, your model may be starting to <strong>overfit</strong>. Consider restarting your training experiment with one fewer epochs than it took to overfit.</p>
</blockquote>
<div class="cell" data-cellview="form" data-outputid="a3411085-9533-4141-81bf-d24565eb9589" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>learn.recorder.plot_loss()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Image_Classification_Tutorial_files/figure-html/cell-8-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="testing-the-model" class="level2">
<h2 class="anchored" data-anchor-id="testing-the-model">Testing the Model</h2>
<section id="test-the-model-on-the-test-dataset" class="level3">
<h3 class="anchored" data-anchor-id="test-the-model-on-the-test-dataset">Test the model on the test dataset</h3>
<p>When you run the following cell, the first line shows the groundtruth for whether the radiograph is of the chest or abdomen. The second line is the model prediction for whether the image is a chest or abdominal radiograph.</p>
<div class="cell" data-cellview="form" data-outputid="66f872dc-26e8-43bf-cb9b-b9cab0914e14" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>test_files <span class="op">=</span> get_image_files(path<span class="op">/</span><span class="st">'test'</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>test_dl <span class="op">=</span> learn.dls.test_dl(test_files, with_labels<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>learn.show_results(dl<span class="op">=</span>test_dl)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<p><img src="Image_Classification_Tutorial_files/figure-html/cell-9-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="a-little-more-detail-on-the-predictions" class="level3">
<h3 class="anchored" data-anchor-id="a-little-more-detail-on-the-predictions">A little more detail on the predictions</h3>
<p>Running this cell will provide us with the loss on each image, as well as the model‚Äôs predicted probability, which can be thought of as the model‚Äôs confidence in its prediction.</p>
<blockquote class="blockquote">
<p>Note: If the model is correct and completely confident, the loss should be near ‚Äú0.00‚Äù and the probability will be ‚Äú1.00‚Äù, respectively.</p>
</blockquote>
<div class="cell" data-cellview="form" data-outputid="7b16f268-b1a0-4b01-fe36-88ddd68482d5" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>interp <span class="op">=</span> ClassificationInterpretation.from_learner(learn, dl<span class="op">=</span>test_dl)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>interp.plot_top_losses(k<span class="op">=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<p><img src="Image_Classification_Tutorial_files/figure-html/cell-10-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="test-the-model-on-a-surprise-example" class="level3">
<h3 class="anchored" data-anchor-id="test-the-model-on-a-surprise-example">Test the model on a surprise example</h3>
<p>Here, we present the model with an unexpected image (an elbow radiograph) and see how it responds.</p>
<div class="cell" data-cellview="form" data-outputid="0499f468-e5ce-4f12-e985-73bc8eca8358" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> get_image_files(path, recurse<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>test_dl <span class="op">=</span> learn.dls.test_dl(y)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>x, <span class="op">=</span> first(test_dl)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> learn.get_preds(dl<span class="op">=</span>test_dl, with_decoded<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>x_dec <span class="op">=</span> TensorImage(dls.train.decode((x,))[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'Prediction / Probability'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>x_dec.show(ctx<span class="op">=</span>ax)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="ss">f'</span><span class="sc">{</span>dls<span class="sc">.</span>vocab[res[<span class="dv">2</span>][<span class="dv">0</span>]]<span class="sc">}</span><span class="ss"> / </span><span class="sc">{</span><span class="bu">max</span>(res[<span class="dv">0</span>][<span class="dv">0</span>])<span class="sc">:.2f}</span><span class="ss">'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

</div>
<div class="cell-output cell-output-display">
<p><img src="Image_Classification_Tutorial_files/figure-html/cell-11-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>When presented with this radiograph of an elbow, the model makes a prediction but is less confident than with the other test images.</p>
<blockquote class="blockquote">
<p>Important: (1) A deep learning classification model <em>can only learn what we teach it to learn</em>.</p>
</blockquote>
<blockquote class="blockquote">
<p>Important: (2) In designing our model implementation, we might consider designing a pre-processing step in which the data (or metadata) is checked to ensure the input to the model is valid.</p>
</blockquote>
</section>
</section>
<section id="visualizing-model-inferences" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-model-inferences">Visualizing Model Inferences</h2>
<section id="class-activation-map-cam" class="level3">
<h3 class="anchored" data-anchor-id="class-activation-map-cam">Class activation map (CAM)</h3>
<p>CAM allows one to visualize which regions of the original image are heavily weighted in the prediction of the corresponding class. This technique provides a visualization of the activations in the <strong>final</strong> <em>convolutional</em> block of a Convolutional Neural Network (CNN).</p>
<p>CAM can also be useful to determine if the model is ‚Äúcheating‚Äù and looking somewhere it shouldn‚Äôt be to make its prediction (i.e.&nbsp;radioopaque markers placed by the technologist).</p>
<blockquote class="blockquote">
<p>Note: If you are running this cell in Colab, choose which of the two test images you would like to examine and run this cell to see the CAM output overlayed on the input image.</p>
</blockquote>
<div class="cell" data-cellview="form" data-outputid="628b3b94-c82d-4496-fa48-f0695d78177a" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>test_case <span class="op">=</span> <span class="st">'chest'</span> <span class="co">#@param ['abd', 'chest']</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>cls <span class="op">=</span> <span class="dv">0</span> <span class="cf">if</span> test_case <span class="op">==</span> <span class="st">'abd'</span> <span class="cf">else</span> <span class="dv">1</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>label <span class="op">=</span> test_case</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> get_image_files(path<span class="op">/</span><span class="st">'test'</span><span class="op">/</span>label)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>test_dl <span class="op">=</span> learn.dls.test_dl(y, with_labels<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>hook <span class="op">=</span> hook_output(learn.model[<span class="dv">0</span>])</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>x, _ <span class="op">=</span> first(test_dl)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad(): output <span class="op">=</span> learn.model.<span class="bu">eval</span>()(x)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>act <span class="op">=</span> hook.stored[<span class="dv">0</span>]</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>cam_map <span class="op">=</span> torch.einsum(<span class="st">'ck,kij-&gt;cij'</span>, learn.model[<span class="dv">1</span>][<span class="op">-</span><span class="dv">1</span>].weight, act)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>x_dec <span class="op">=</span> TensorImage(dls.train.decode((x,))[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>_, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>x_dec.show(ctx<span class="op">=</span>ax)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>ax.imshow(cam_map[cls].detach().cpu(), alpha<span class="op">=</span><span class="fl">0.6</span>, extent<span class="op">=</span>(<span class="dv">0</span>,<span class="dv">224</span>,<span class="dv">224</span>,<span class="dv">0</span>),</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>              interpolation<span class="op">=</span><span class="st">'bilinear'</span>, cmap<span class="op">=</span><span class="st">'magma'</span>)<span class="op">;</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>hook.remove()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Image_Classification_Tutorial_files/figure-html/cell-12-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="grad-cam" class="level3">
<h3 class="anchored" data-anchor-id="grad-cam">Grad-CAM</h3>
<p>Gradient-weighted CAM (Grad-CAM) allows us to visualize the output from <em>any convolutional block</em> in a CNN.</p>
<p>By default, this cell is setup to show the Grad-CAM output from the final convolutional block in the CNN, for comparison to the CAM output.</p>
<blockquote class="blockquote">
<p>Note: If you‚Äôre running this notebook in Colab, (1) choose which of the two test images you would like to examine and run this cell to see the Grad-CAM output overlayed on the input image, then (2) select a <em>different</em> block and re-run the cell to see how the output changes for different blocks in the network.</p>
</blockquote>
<div class="cell" data-cellview="form" data-outputid="be4fe3eb-4860-428f-ab08-99877b542c51" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>test_case <span class="op">=</span> <span class="st">'abd'</span> <span class="co">#@param ['abd', 'chest']</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>cls <span class="op">=</span> <span class="dv">0</span> <span class="cf">if</span> test_case <span class="op">==</span> <span class="st">'abd'</span> <span class="cf">else</span> <span class="dv">1</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>label <span class="op">=</span> test_case</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> get_image_files(path<span class="op">/</span><span class="st">'test'</span><span class="op">/</span>label)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>test_dl <span class="op">=</span> learn.dls.test_dl(y, with_labels<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>x, _ <span class="op">=</span> first(test_dl)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>mod <span class="op">=</span> learn.model[<span class="dv">0</span>]</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>block <span class="op">=</span> <span class="op">-</span><span class="dv">2</span> <span class="co">#@param {type: "slider", min: -8, max: -1, step: 1}</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>hook_func <span class="op">=</span> <span class="kw">lambda</span> m,i,o: o[<span class="dv">0</span>].detach().clone()</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> Hook(mod[block], hook_func, is_forward<span class="op">=</span><span class="va">False</span>) <span class="im">as</span> hookg:</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> Hook(mod[block], hook_func) <span class="im">as</span> hook:</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> learn.model.<span class="bu">eval</span>()(x.cuda())</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        act <span class="op">=</span> hook.stored</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    output[<span class="dv">0</span>, cls].backward()</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    grad <span class="op">=</span> hookg.stored</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> grad[<span class="dv">0</span>].mean(dim<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">2</span>], keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>cam_map <span class="op">=</span> (w <span class="op">*</span> act[<span class="dv">0</span>]).<span class="bu">sum</span>(<span class="dv">0</span>)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>x_dec <span class="op">=</span> TensorImage(dls.train.decode((x,))[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>_, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>x_dec.show(ctx<span class="op">=</span>ax)</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>ax.imshow(cam_map.detach().cpu(), alpha<span class="op">=</span><span class="fl">0.6</span>, extent<span class="op">=</span>(<span class="dv">0</span>,<span class="dv">224</span>,<span class="dv">224</span>,<span class="dv">0</span>),</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>              interpolation<span class="op">=</span><span class="st">'bilinear'</span>, cmap<span class="op">=</span><span class="st">'magma'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Image_Classification_Tutorial_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"009c10761d2f427e8f63c9cbac746fe3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bc5107098274f61954b854197a4543f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"3afe510aab3d4134a67377da6b8318fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42e792826caa4935b7adbf5432268259":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ecef27e90bcf47e091c20b26b0b51262","IPY_MODEL_fd9acb21e1a9427e823a0fe083694507"],"layout":"IPY_MODEL_922be60cbdab493fa7fb4e09c1ec2d09"}},"922be60cbdab493fa7fb4e09c1ec2d09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4b6416702fe450bad03f213c74eef8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecef27e90bcf47e091c20b26b0b51262":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_009c10761d2f427e8f63c9cbac746fe3","max":46827520,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0bc5107098274f61954b854197a4543f","value":46827520}},"fd9acb21e1a9427e823a0fe083694507":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4b6416702fe450bad03f213c74eef8a","placeholder":"‚Äã","style":"IPY_MODEL_3afe510aab3d4134a67377da6b8318fe","value":" 44.7M/44.7M [00:09&lt;00:00, 4.80MB/s]"}}}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>