[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Educational Resources",
    "section": "",
    "text": "I’ve put together a curated list of resources with a mind for what is most useful and efficient for time-constrained residents, fellows, or attending radioloigsts. I have tried to grade the accessibility of the content as “Novice”, “Beginner”, or “Intermediate”, though your experience may vary. I will make an effort to update this list over time.\n\n\n\n\nMy definitions of experience\n\n\n\n\nNovice\n\n\nLimited or no prior coding experience\n\n\n\n\nBeginner\n\n\nSome prior coding experience, perhaps in another language (aside from Python) or unrelated to Data Science and Deep Learning\n\n\n\n\nIntermediate\n\n\nBasic familiarity with Python for Data Science and Deep Learning\n\n\n\n\n\n\n\n\nKaggle Intro to Python\n\n\n\nA 7-part introduction to the Python programming language, taught in the Kaggle Notebook self-contained programming environment.\n\n\n\nKaggle 7-day Python Challenge\n\n\n\nA 7-day challenge for learning the basics of Python programming in the Kaggle Notebook (formerly known as “Kernel”) self-contained programming environment.\n\n\n\nPython for Non-Programmers\n\n\n\nAn extensive list of interactive tutorials and other online resources for learning the basics of programming in Python\n\n\n\nRadioGraphics - Deep Learning: A Primer for Radiologists\n\n\n\nPublished in 2017, this general review of deep learning covers basic terminology and concepts with examples of applications to image analysis.\n\n\n\nKaggle Notebook - Hello World for Deep Learning in Medical Imaging\n\n\n\nSelf-guided tutorial for deep learning in medical imaging using Python\n\n\nNo coding experience required\n\n\nWalkthrough of code training a model to differentiate chest and abdominal radiographs\n\n\n\n\n\n\n\n\nPython for Data Analysis by Wes McKinney (Amazon)\n\n\n\nBeginner level book covering basic topics in data science\n\n\nEmploys common Python libraries: iPython, NumPy, Pandas and matplotlib\n\n\n\nDeep Learning with Python by Francois Chollet (Amazon)\n\n\n\nBeginner level book covering basic topics in deep learning and computer vision\n\n\nWritten by the author of the Keras framework for TensorFlow - using Keras, now known as tf.keras\n\n\n\nThe Missing Semester of Your CS Education - MIT\n\n\n\nA series of courses covering the basics of computing tools often used by programmers such as the command line interface (CLI or shell) and Git (version control).\n\n\n\nPython for Neuroimaging for Beginners by Kevin Cho\n\n\n\nSeries of 4 YouTube videos covering basic Python functions for manipulating and processing image data.\n\n\nGeared toward neuroimaging, but skills are generalizable to all imaging subspecialties.\n\n\n\n\n\n\n\n\nfast.ai Practical Deep Learning for Coders\n\n\n\n“Code first, theory later” series of tutorials in deep learning\n\n\nCovers computer vision, natural language processing and tabular learning\n\n\nAdvanced course follows, delving deeper into fundamentals and theory of deep learning\n\n\n\nUC-Irvine CS190 Course: Deep Learning for Medical Imaging by Dr. Peter Chang (link to GitHub repository)\n\n\n\nComprehensive introductory course with focus on medical imaging utilizing Python notebooks in Google Colab with the TensorFlow 2.0/Keras API.\n\n\nLinks to tutorial videos and slides included in README.md file.\n\n\nCourse notebooks can be downloaded from notebooks folder and uploaded to Google Colab.\n\n\n\nMore to come…"
  },
  {
    "objectID": "resources.html#novice-resources",
    "href": "resources.html#novice-resources",
    "title": "Educational Resources",
    "section": "",
    "text": "Kaggle Intro to Python\n\n\n\nA 7-part introduction to the Python programming language, taught in the Kaggle Notebook self-contained programming environment.\n\n\n\nKaggle 7-day Python Challenge\n\n\n\nA 7-day challenge for learning the basics of Python programming in the Kaggle Notebook (formerly known as “Kernel”) self-contained programming environment.\n\n\n\nPython for Non-Programmers\n\n\n\nAn extensive list of interactive tutorials and other online resources for learning the basics of programming in Python\n\n\n\nRadioGraphics - Deep Learning: A Primer for Radiologists\n\n\n\nPublished in 2017, this general review of deep learning covers basic terminology and concepts with examples of applications to image analysis.\n\n\n\nKaggle Notebook - Hello World for Deep Learning in Medical Imaging\n\n\n\nSelf-guided tutorial for deep learning in medical imaging using Python\n\n\nNo coding experience required\n\n\nWalkthrough of code training a model to differentiate chest and abdominal radiographs"
  },
  {
    "objectID": "resources.html#beginner-resources",
    "href": "resources.html#beginner-resources",
    "title": "Educational Resources",
    "section": "",
    "text": "Python for Data Analysis by Wes McKinney (Amazon)\n\n\n\nBeginner level book covering basic topics in data science\n\n\nEmploys common Python libraries: iPython, NumPy, Pandas and matplotlib\n\n\n\nDeep Learning with Python by Francois Chollet (Amazon)\n\n\n\nBeginner level book covering basic topics in deep learning and computer vision\n\n\nWritten by the author of the Keras framework for TensorFlow - using Keras, now known as tf.keras\n\n\n\nThe Missing Semester of Your CS Education - MIT\n\n\n\nA series of courses covering the basics of computing tools often used by programmers such as the command line interface (CLI or shell) and Git (version control).\n\n\n\nPython for Neuroimaging for Beginners by Kevin Cho\n\n\n\nSeries of 4 YouTube videos covering basic Python functions for manipulating and processing image data.\n\n\nGeared toward neuroimaging, but skills are generalizable to all imaging subspecialties."
  },
  {
    "objectID": "resources.html#intermediate-resources",
    "href": "resources.html#intermediate-resources",
    "title": "Educational Resources",
    "section": "",
    "text": "fast.ai Practical Deep Learning for Coders\n\n\n\n“Code first, theory later” series of tutorials in deep learning\n\n\nCovers computer vision, natural language processing and tabular learning\n\n\nAdvanced course follows, delving deeper into fundamentals and theory of deep learning\n\n\n\nUC-Irvine CS190 Course: Deep Learning for Medical Imaging by Dr. Peter Chang (link to GitHub repository)\n\n\n\nComprehensive introductory course with focus on medical imaging utilizing Python notebooks in Google Colab with the TensorFlow 2.0/Keras API.\n\n\nLinks to tutorial videos and slides included in README.md file.\n\n\nCourse notebooks can be downloaded from notebooks folder and uploaded to Google Colab.\n\n\n\nMore to come…"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Basic Setup Guide",
    "section": "",
    "text": "This guide will step you through the process of setting up your development environment for using the command line to run programs in Python. The ideal operating system (OS) to use is a Unix-based system such as MacOS or a Linux distribution (e.g. Ubuntu). For simplicity, I’ll assume you’re using MacOS, as Linux users probably won’t need many of these tutorials.\nFor users on Windows 10 or 11, your best bet is to install the Windows Subsystem for Linux (WSL).\n\n\n\n\n\n\nNote\n\n\n\nI recommend selecting the Ubuntu distribution directly from the Windows Store:  WSL Ubuntu Installation Tutorial.\n\n\nWSL should afford a similar experience to MacOS, though you may end up spending a little more time with Google and Stack Overflow to get certain elements of the your development environment up and running.\n\nInstalling Developer Tools\n\nAgain, from here on out, we’re assuming that you’re using MacOS… I’ll try to add support for WSL as this site evolves.\nThe first step is to open up a terminal window. The easiest way to do this is to launch the Spotlight app via the keyboard shortcut {CMD-SPACE}, then type “terminal”, followed by {ENTER}.\n\n\n\n\n\n\nNote\n\n\n\nIf you haven’t already, this is the perfect time to head over to the Intro to CLI post for an introduction to the Command Line Interface (CLI or “shell”).\n\n\nNow, the next thing we need to do is install the Command Line Tools for Xcode. This will ensure that you have the full range of command line tools available to you - a necessary step before Advanced Setup. Enter the following command into your terminal window.\nxcode-select --install\n\nText Editors\n\nEvery programmer has his or her preferred text editor. I use VS Code. I used to use Sublime Text. I dabble in vim for when I want to quickly edit a config file or a shell script without leaving my Terminal. I don’t recommend vim to beginners. VS Code is free and works great out of the box and is very configurable/extensible. Do a little research for yourself and figure out which text editor might be best for you. Just pick a nice one with some simple features like syntax highlighting and it will make programming/coding a lot more fun…and efficient.\nIf you plan to do the Advanced Setup, this would be a good time to stop and head over there before continuing with these steps.\nIf not, feel free to proceed.\n\nOption 1: Installing Python with Anaconda\n\nThough Python comes preinstalled with recent versions of MacOS, I highly recommend using a package and environment manager such as Anaconda (or Miniconda) for managing Python packages and virutal environments (more on these later). You could follow this link to download and install Anaconda with all of it’s bells and whistles. However, since we’re learning the command line, I recommend you go with Miniconda, a lighter-weight version without the Spyder independent development environment (IDE).\nThe only potential downside of installing a version of conda is that Pytorch has recently announced that they are deprecating support for conda packages. Meaning that if you’ve already been using Pytorch, there may be a point in the near future that you need a different option. If you think this might be the case for you, then I suggest giving uv a try (see Option 2 below).\nIf you go with Miniconda, enter the following commands into your terminal window from your home directory. These come from the Miniconda Quick Command Line Install guide and are current as of 20 December 2024. I’ve added them here so I can explain what’s happening. The first command uses mkdir to create a subdirectory of your home folder called miniconda. The -p flag ensures that no error occurs if the directory already exists and will create any “parent” directories, if necessary. The second command uses curl to download the installation file. The -o option renames the file to the shorter miniconda.sh and downloads that file to the miniconda directory. The third command uses the bash shell to run the miniconda.sh shell script, which will install miniconda. The final command cleans up the install file with the rm command (“remove”), since it is no longer needed.\nmkdir -p ~/miniconda3\ncurl https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh -o ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nrm ~/miniconda3/miniconda.sh\nFollow any prompts that arise during the install process. Once this is complete, the next thing you’ll want to do is restart your terminal. Then test whether the install worked by entering a conda command (e.g. conda list). If this produces an error, then there are a couple of possibilities as to why this may not have worked, most of which should be fixed after the next step.\nThe next step is to check your shell config file, either ~/.bashrc or ~/.zshrc. Open this file in nano or your favorite graphical text editor. The Miniconda install process should have appended the following line to your “rc” file: export PATH=“~/miniconda3/bin:$PATH”. This command prepends the path to the Miniconda functions to your system path environment variable. If this is not the case, then you should add this line manually, save your changes and reload your shell (open a new terminal window or source your rc file).\n\nOption 2: Installing Python with uv\n\nAnother option for installing a managed version of Python is uv. The recommended way to install this package is via the following command from the uv Installation guide, and is current as of 20 December 2024.\nThis command uses curl to download the installation shell script, then “pipes” the file to the sh shell, which runs the script.\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nFollow any instructions during the installation process. Once it is complete, close and reopen your terminal window to “restart” your shell and then run the following command to install the latest stable version of Python.\nuv python install\nIf you need to install a specific version of Python for compatibility with existing code or a tutorial you’re following, then use the following command and change the version number to match your needs.\nuv python install 3.12\n\nCongrats!\n\nYou’ve completed Basic Setup. If you haven’t already, I recommend proceeding to the Advanced Setup guide… or go straight to learning more about the command line, shell scripting, and Python!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Walter Wiggins, MD, PhD",
    "section": "",
    "text": "I’m a neuroradiologist who likes to code. By day, I split my time between (1) taking care of patients by interpreting diagnostic imaging of the brain, head, neck, and spine, as well as doing image-guided, minimally invasive procedures; and (2) working hard to deploy and monitor AI tools for radiology.\nEducation around AI and machine learning is vital for radiologists. While we don’t need to understand everything about how these tools work, I think it helps to understand a little about how “the sausage is made” in order to use these tools effectively. Just like diagnostic imaging modalities, you should have a basic understanding of AI’s uses and limitations, as well as some common ways it can fail.\nThis blog is dedicated to interactive tutorials covering various topics in AI & ML for medical imaging and natural language processing (NLP) or text analysis.\nI hope you enjoy!\nCheers, Walter"
  },
  {
    "objectID": "posts/RNNs_tutorial.html",
    "href": "posts/RNNs_tutorial.html",
    "title": "RNNs Tutorial from RSNA 2021 Deep Learning Lab",
    "section": "",
    "text": "In this demonstration, we will utilize techniques of natural language processing (NLP) to train a classifier, which will analyze the text of radiology reports for chest radiographs to predict whether a report is normal or abnormal.\n\n\nWe will utilize the fast.ai v2 library, written primarily by Jeremy Howard and Sylvain Gugger (with help from many others). It is written in the Python programming language and built on top of the PyTorch deep learning library.\nThe demonstration in this notebook relies heavily on examples from the fast.ai book, Deep Learning for Coders with fastai and PyTorch: AI Applications without a PhD by Jeremy Howard and Sylvain Gugger, which was written entirely in Jupyter notebooks, which are freely available for download on GitHub. A print copy of the book can be purchased from Amazon.\n\n\n\nThe data is obtained from the National Library of Medicine’s Open-i service. We utilize the radiology reports from the Indiana University Chest X-ray Dataset for this demonstration.\n\nReference: Demner-Fushman D, Kohli MD, Rosenman MB, Shooshan SE, Rodriguez L, Antani S, Thoma GR, McDonald CJ. Preparing a collection of radiology examinations for distribution and retrieval. J Am Med Inform Assoc. 2016 Mar;23(2):304-10. doi: 10.1093/jamia/ocv080. Epub 2015 Jul 1.\n\n\n\n\n\nWalter F. Wiggins, MD, PhD - Duke University Hospital, Durham, NC, USA\nFelipe Kitamura, MD - UNIFESP, Sao Paulo, Brasil\nIgor Santos, MD - UNIFESP, Sao Paulo, Brasil\nLuciano M. Prevedello, MD, MPH - Ohio State University, Columbus, OH, USA\n\n\n\n\n\nThis notebook is designed for interactive participation.\nIf you are running this notebook on your own machine, you may download the notebook with this link: Download notebook.\nPlease refer to the previous posts on installing Python and installing PyTorch and fast.ai with uv to ensure your environment is set up properly. You’ll also need to run the following commands to add two additional packages to your uv project.\nuv add xmltodict\nuv add tensorboard\nYou can run the code cells in order, and modify parameters as desired to see how they affect the results.\nIf you are using Google Colab, please use the link below to open a compatible version in Colab. You can save a copy of the notebook to your own Google Drive by selecting “File” -&gt; “Save a copy in Drive”.\nOpen in Colab"
  },
  {
    "objectID": "posts/RNNs_tutorial.html#basics-of-information-extraction-from-radiology-reports",
    "href": "posts/RNNs_tutorial.html#basics-of-information-extraction-from-radiology-reports",
    "title": "RNNs Tutorial from RSNA 2021 Deep Learning Lab",
    "section": "",
    "text": "In this demonstration, we will utilize techniques of natural language processing (NLP) to train a classifier, which will analyze the text of radiology reports for chest radiographs to predict whether a report is normal or abnormal.\n\n\nWe will utilize the fast.ai v2 library, written primarily by Jeremy Howard and Sylvain Gugger (with help from many others). It is written in the Python programming language and built on top of the PyTorch deep learning library.\nThe demonstration in this notebook relies heavily on examples from the fast.ai book, Deep Learning for Coders with fastai and PyTorch: AI Applications without a PhD by Jeremy Howard and Sylvain Gugger, which was written entirely in Jupyter notebooks, which are freely available for download on GitHub. A print copy of the book can be purchased from Amazon.\n\n\n\nThe data is obtained from the National Library of Medicine’s Open-i service. We utilize the radiology reports from the Indiana University Chest X-ray Dataset for this demonstration.\n\nReference: Demner-Fushman D, Kohli MD, Rosenman MB, Shooshan SE, Rodriguez L, Antani S, Thoma GR, McDonald CJ. Preparing a collection of radiology examinations for distribution and retrieval. J Am Med Inform Assoc. 2016 Mar;23(2):304-10. doi: 10.1093/jamia/ocv080. Epub 2015 Jul 1.\n\n\n\n\n\nWalter F. Wiggins, MD, PhD - Duke University Hospital, Durham, NC, USA\nFelipe Kitamura, MD - UNIFESP, Sao Paulo, Brasil\nIgor Santos, MD - UNIFESP, Sao Paulo, Brasil\nLuciano M. Prevedello, MD, MPH - Ohio State University, Columbus, OH, USA"
  },
  {
    "objectID": "posts/RNNs_tutorial.html#interactive-participation",
    "href": "posts/RNNs_tutorial.html#interactive-participation",
    "title": "RNNs Tutorial from RSNA 2021 Deep Learning Lab",
    "section": "",
    "text": "This notebook is designed for interactive participation.\nIf you are running this notebook on your own machine, you may download the notebook with this link: Download notebook.\nPlease refer to the previous posts on installing Python and installing PyTorch and fast.ai with uv to ensure your environment is set up properly. You’ll also need to run the following commands to add two additional packages to your uv project.\nuv add xmltodict\nuv add tensorboard\nYou can run the code cells in order, and modify parameters as desired to see how they affect the results.\nIf you are using Google Colab, please use the link below to open a compatible version in Colab. You can save a copy of the notebook to your own Google Drive by selecting “File” -&gt; “Save a copy in Drive”.\nOpen in Colab"
  },
  {
    "objectID": "posts/RNNs_tutorial.html#tokenization",
    "href": "posts/RNNs_tutorial.html#tokenization",
    "title": "RNNs Tutorial from RSNA 2021 Deep Learning Lab",
    "section": "Tokenization",
    "text": "Tokenization\nTokenization is the process by which we split text into chunks or tokens. An individual token could be a sentence or phrase, a word, a part of a word, or a single character. Most often, we choose to tokenize at the word or sub-word level.\nEach token is assigned a sequential integer value and the collection of token-integer pairs is called our vocabulary.\n\n\nCode\n#@title **Word Tokenization**\n\nspacy = WordTokenizer()\ntext = train_val.iloc[0]['full-text']\nprint('Original text:')\nprint(text)\nprint()\ntkns = first(spacy([text]))\nprint('After word tokenization:')\nprint(coll_repr(tkns))\n\n\nOriginal text:\nThe XXXX examination consists of frontal and lateral radiographs of the chest. The cardiac silhouette is not enlarged. There has been apparent interval increase in low density convexity at the left cardiophrenic XXXX. Calcified granuloma is again seen in the right upper lobe. There is no consolidation, pleural effusion or pneumothorax. Increased size of density in the left cardiophrenic XXXX. Primary differential considerations include increased size of prominent epicardial fat, pericardial mass, pleural mass or cardiac aneurysm. CT chest with contrast is recommended. These findings and recommendations were discussed XXXX. XXXX by Dr. XXXX XXXX telephone at XXXX p.m. XXXX/XXXX. Dr. XXXX&lt;XXXX&gt;technologist receipt of the results.\n\nAfter word tokenization:\n['The', 'XXXX', 'examination', 'consists', 'of', 'frontal', 'and', 'lateral', 'radiographs', 'of', 'the', 'chest', '.', 'The', 'cardiac', 'silhouette', 'is', 'not', 'enlarged', '.', 'There', 'has', 'been', 'apparent', 'interval', 'increase', 'in', 'low', 'density', 'convexity', 'at', 'the', 'left', 'cardiophrenic', 'XXXX', '.', 'Calcified', 'granuloma', 'is', 'again', 'seen', 'in', 'the', 'right', 'upper', 'lobe', '.', 'There', 'is', 'no', 'consolidation', ',', 'pleural', 'effusion', 'or', 'pneumothorax', '.', 'Increased', 'size', 'of', 'density', 'in', 'the', 'left', 'cardiophrenic', 'XXXX', '.', 'Primary', 'differential', 'considerations', 'include', 'increased', 'size', 'of', 'prominent', 'epicardial', 'fat', ',', 'pericardial', 'mass', ',', 'pleural', 'mass', 'or', 'cardiac', 'aneurysm', '.', 'CT', 'chest', 'with', 'contrast', 'is', 'recommended', '.', 'These', 'findings', 'and', 'recommendations', 'were', 'discussed', 'XXXX', '.', 'XXXX', 'by', 'Dr.', 'XXXX', 'XXXX', 'telephone', 'at', 'XXXX', 'p.m.', 'XXXX', '/', 'XXXX', '.', 'Dr.', 'XXXX', '&lt;', 'XXXX', '&gt;', 'technologist', 'receipt', 'of', 'the', 'results', '.']"
  },
  {
    "objectID": "posts/RNNs_tutorial.html#added-features-from-fast.ai",
    "href": "posts/RNNs_tutorial.html#added-features-from-fast.ai",
    "title": "RNNs Tutorial from RSNA 2021 Deep Learning Lab",
    "section": "Added features from fast.ai",
    "text": "Added features from fast.ai\nYou’ll notice some odd-appearing tokens in the output from the next cell. These are special tokens that indicate certain things about the text. - 'xxbos' indicates the beginning of the text stream - 'xxmaj' indicates that the following character was capitalized before fast.ai lowered it - 'xxrep' followed by '4', 'x' means that 'x' is repeated 4 times…the XXXX in the reports is a product of the anonymization process that the team who generated the dataset used\nThese special tokens enrich the data while reducing the vocab by eliminating redundant upper and lower case variants of individual words.\n\n\nCode\n#@title **fast.ai Tokenization**\n\ntkn = Tokenizer(spacy)\ntoks = tkn(text)\nprint(coll_repr(toks, 15))\n\n\n(#164) ['xxbos', 'xxmaj', 'the', 'xxrep', '4', 'x', 'examination', 'consists', 'of', 'frontal', 'and', 'lateral', 'radiographs', 'of', 'the'...]"
  },
  {
    "objectID": "posts/RNNs_tutorial.html#numericalization",
    "href": "posts/RNNs_tutorial.html#numericalization",
    "title": "RNNs Tutorial from RSNA 2021 Deep Learning Lab",
    "section": "Numericalization",
    "text": "Numericalization\nAfter converting text to tokens, the next step is to convert each unique token to a number.\nFor language modeling, we should do this on all of the text that we might use for training and validation. So we’ll carry this out on the combined Findings and Impression for each of our reports and tokenize the text to define our vocabulary. Each token in the vocabulary will be identified by a unique number.\n\n\nCode\ntxts = L(train_val['full-text'].to_list())\ntoks = txts.map(tkn)\nnum = Numericalize()\nnum.setup(toks)\ncoll_repr(num.vocab, 20)\n\n\n\"(#1192) ['xxunk', 'xxpad', 'xxbos', 'xxeos', 'xxfld', 'xxrep', 'xxwrep', 'xxup', 'xxmaj', '.', 'no', 'the', 'is', 'are', 'normal', ',', '4', 'x', 'of', 'and'...]\"\n\n\nAs you can see above, we have 1,192 tokens in our vocabulary. The special tokens from fast.ai appear first, followed by every other token in order of decreasing frequency. By default, fast.ai only includes tokens that appear at least 3 times in the corpus (collection of texts).\n\n\nCode\nnums = num(toks[0][:14])\nprint(nums)\nprint(' '.join(num.vocab[i] for i in nums))\n\n\nTensorText([  2,   8,  11,   5,  16,  17, 162, 314,  18, 178,  19,  89, 261,\n             18])\nxxbos xxmaj the xxrep 4 x examination consists of frontal and lateral radiographs of\n\n\nHere, we see a subset of the numericalized tokens (top) with their corresponding word tokens (bottom) from the first report in our corpus."
  },
  {
    "objectID": "posts/RNNs_tutorial.html#embedding",
    "href": "posts/RNNs_tutorial.html#embedding",
    "title": "RNNs Tutorial from RSNA 2021 Deep Learning Lab",
    "section": "Embedding",
    "text": "Embedding\nWhile these integers map one-to-one onto our tokens, their numeric value is otherwise meaningless. To embed more information into the numeric representation of our tokens, we employ a process called language modeling. We can either use a pre-trained language model for this embedding or we can fine-tune a language model to better model our “radiology language”. This latter process is called transfer learning."
  },
  {
    "objectID": "posts/RNNs_tutorial.html#language-model-transfer-learning",
    "href": "posts/RNNs_tutorial.html#language-model-transfer-learning",
    "title": "RNNs Tutorial from RSNA 2021 Deep Learning Lab",
    "section": "Language Model Transfer Learning",
    "text": "Language Model Transfer Learning\n\nGiven a string of tokenized text, a language model is trained to predict the next token.\n\nBy default, fastai.text uses a language model pre-trained on the WikiText-103 corpus, which is a frequently used benchmark for NLP models, similar to ImageNet for image classification models.\nIn their 2018 ACL paper “Universal Language Model Fine-Tuning for Text Classification”, Jeremy Howard and Sebastian Ruder proposed the ULM-FiT approach of fine-tuning the language model on the corpus on which a second text classification model is trained. In the paper, they demonstrated state-of-the-art performance on a commonly used text classification example using this method.\nHere, we will fine-tune the language model on our radiology report text in two stages.\n\n\n\n\n\n\nNote\n\n\n\nThis portion of the notebook takes between 3 and 10 minutes to run.\n\n\n\n\nCode\ndls_lm = TextDataLoaders.from_df(train_val, valid_pct=0.3, seed=42, text_col='full-text', is_lm=True)\nlearn_lm = language_model_learner(dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()]).to_fp16()\n\n# Train the final layers of the network\nlearn_lm.fit_one_cycle(1, 2e-2)\nprint(\"\\nPart 1/2 complete.\\n\")\n\n# Unfreeze and train some more\nlearn_lm.unfreeze()\nlearn_lm.fit_one_cycle(10, 2e-3)\nprint(\"\\nPart 2/2 complete.\\n\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n3.243420\n2.253190\n0.521906\n9.518048\n00:06\n\n\n\n\n\n\nPart 1/2 complete.\n\n\nPart 2/2 complete.\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n2.383461\n2.023076\n0.560477\n7.561547\n00:06\n\n\n1\n2.197973\n1.785538\n0.604102\n5.962786\n00:07\n\n\n2\n2.016547\n1.625365\n0.632902\n5.080271\n00:06\n\n\n3\n1.852749\n1.529698\n0.654709\n4.616784\n00:06\n\n\n4\n1.727462\n1.485514\n0.664200\n4.417233\n00:06\n\n\n5\n1.613133\n1.435348\n0.674501\n4.201105\n00:06\n\n\n6\n1.520230\n1.406741\n0.680834\n4.082631\n00:06\n\n\n7\n1.441349\n1.394186\n0.684216\n4.031690\n00:06\n\n\n8\n1.376257\n1.386329\n0.686632\n4.000140\n00:06\n\n\n9\n1.340578\n1.385572\n0.687009\n3.997111\n00:07\n\n\n\n\n\nWe need to save this model state in a special format in order to use it for our Embedding layer in the text classification model.\nWe also want to use the same vocabulary that we used for the ULM-FiT procedure in our classification model.\n\n\nCode\ntorch.save(learn_lm.dls.vocab, 'vocab.pkl')\nlearn_lm.save_encoder('fine-tuned-enc')\nprint('Saved.')\n\n\nSaved."
  },
  {
    "objectID": "posts/RNNs_tutorial.html#iterative-training-process",
    "href": "posts/RNNs_tutorial.html#iterative-training-process",
    "title": "RNNs Tutorial from RSNA 2021 Deep Learning Lab",
    "section": "Iterative Training Process",
    "text": "Iterative Training Process\nSo that we don’t propagate too much error through the network during training, we will train the final layers first, then iteratively unfreeze (i.e. make trainable) a few preceding layers and train a little more before repeating this process.\n\n\nCode\nlearn_cls = text_classifier_learner(dls_cls, AWD_LSTM, drop_mult=0.5, metrics=accuracy).to_fp16()\nif fine_tuned_LM: learn_cls.load_encoder('fine-tuned-enc')\nlearn_cls.fit_one_cycle(1, 2e-2)\nprint(\"\\nPart 1/4 complete.\\n\")\n\n# Unfreezing and training a little more...\nlearn_cls.freeze_to(-2)\nlearn_cls.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\nprint(\"\\nPart 2/4 complete.\\n\")\n\n# Unfreezing and training a little more, again...\nlearn_cls.freeze_to(-3)\nlearn_cls.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\nprint(\"\\nPart 3/4 complete.\\n\")\n\n# Now let's unfreeze the whole model and finish training.\nlearn_cls.unfreeze()\nlearn_cls.fit_one_cycle(1, slice(1e-3/(2.6**4),1e-3))\nprint(\"\\nPart 4/4 complete.\")\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.529666\n0.375562\n0.922619\n00:05\n\n\n\n\n\n\nPart 1/4 complete.\n\n\nPart 2/4 complete.\n\n\nPart 3/4 complete.\n\n\nPart 4/4 complete.\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.487774\n0.255552\n0.939484\n00:04\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.380615\n0.170027\n0.948413\n00:05\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.297941\n0.143385\n0.955357\n00:07\n\n\n\n\n\n\n\nCode\n# Export fine-tuned classifier model\n\nlearn_cls.export('full-text')"
  },
  {
    "objectID": "posts/RNNs_tutorial.html#tensorboard",
    "href": "posts/RNNs_tutorial.html#tensorboard",
    "title": "RNNs Tutorial from RSNA 2021 Deep Learning Lab",
    "section": "Tensorboard",
    "text": "Tensorboard\nRun the following cell, then hit the refresh button in the top right menu of the Tensorboard iframe that appears.\nNext, look along the left hand menu for T-SNE. Click it and let it run for ~1,000 iterations. Then hit the “stop” button.\nNow, explore the embedding space to see how radiological terms cluster in the embedding space of a language model fine-tuned on CXR reports.\n\n\n\nTensorboard T-SNE Embedding Visualization"
  },
  {
    "objectID": "posts/RNNs_tutorial.html#testing-the-model",
    "href": "posts/RNNs_tutorial.html#testing-the-model",
    "title": "RNNs Tutorial from RSNA 2021 Deep Learning Lab",
    "section": "Testing the model",
    "text": "Testing the model\nNow that our model is trained, let’s test it on our held-out test dataset.\nWe will run our report classification model on the held-out test dataset.\nA report will be generated with the following information:\n\nprecision: a.k.a. positive predictive value = true_positives / all_predicted_positives\nrecall: a.k.a. sensitivity = true_positives / all_actual_positives\nf1-score: the harmonic mean of precision and recall\naccuracy: number_correct / number_reports\n\n\n\n\n\n\n\nNote\n\n\n\nSupport = number of reports used to derive the result in that row\n\n\n\n\nCode\ntest_items = test_df[[training_target, 'label']].copy()\ntest_items.columns = ['text', 'label']\ndl_test = learn_cls.dls.test_dl(test_items, with_labels=True)\ninterp = ClassificationInterpretation.from_learner(learn_cls, dl=dl_test)\ninterp.print_classification_report()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n    abnormal       0.97      0.95      0.96       385\n      normal       0.91      0.95      0.93       209\n\n    accuracy                           0.95       594\n   macro avg       0.94      0.95      0.95       594\nweighted avg       0.95      0.95      0.95       594"
  },
  {
    "objectID": "posts/RNNs_tutorial.html#the-confusion-matrix",
    "href": "posts/RNNs_tutorial.html#the-confusion-matrix",
    "title": "RNNs Tutorial from RSNA 2021 Deep Learning Lab",
    "section": "The confusion matrix",
    "text": "The confusion matrix\nThe confusion matrix is a graphical representation of how the model performed. In our case, it is a 2x2 table comparing model predictions to the actual label for each class (i.e. “normal” and “abnormal”).\n\n\nCode\ninterp.plot_confusion_matrix()"
  },
  {
    "objectID": "posts/RNNs_tutorial.html#analyzing-the-failures-or-top-losses",
    "href": "posts/RNNs_tutorial.html#analyzing-the-failures-or-top-losses",
    "title": "RNNs Tutorial from RSNA 2021 Deep Learning Lab",
    "section": "Analyzing the failures or “top losses”",
    "text": "Analyzing the failures or “top losses”\nThe loss function gives us a numerical value telling us how far off the model’s prediction is from the actual class. The higher the loss, the more confused our model is.\nAnalyzing the top losses can help you understand why your model fails when it fails. Here, we plot the top 3 losses for our test dataset.\n\n\nCode\ninterp.plot_top_losses(3)\n\n\n\n\n\n\n\n\n\n\n\n\n\ninput\ntarget\npredicted\nprobability\nloss\n\n\n\n\n0\nxxbos xxmaj the heart , pulmonary xxrep 4 x and mediastinum are within normal limits . xxmaj there is no pleural effusion or pneumothorax . xxmaj there is no focal air space opacity to suggest a pneumonia . xxmaj there is no pulmonary nodule identified . xxmaj there is a left humerus prosthesis xxunk demonstrated . xxmaj no acute cardiopulmonary disease . xxmaj no evidence for metastatic disease by radiographic evaluation .\nnormal\nabnormal\n0.9536540508270264\n3.5140838623046875\n\n\n1\nxxbos xxmaj the heart size and pulmonary vascularity appear within normal limits . xxmaj lungs are free of focal airspace disease . xxmaj no pleural effusion or pneumothorax is seen . xxmaj no discrete nodules or adenopathy are noted . xxmaj degenerative changes are present in the spine . xxmaj no evidence of active disease .\nnormal\nabnormal\n0.9602826833724976\n3.225968599319458\n\n\n2\nxxbos xxmaj heart xxrep 4 x , mediastinum , xxrep 4 x , bony structures and lung xxrep 4 x are unremarkable . xxmaj no radiographic evidence of acute cardiopulmonary disease\nabnormal\nnormal\n0.9702249765396118\n3.071622133255005"
  },
  {
    "objectID": "posts/install-pytorch.html",
    "href": "posts/install-pytorch.html",
    "title": "Managing Projects and Installing Pytorch with uv and light-the-torch",
    "section": "",
    "text": "Let’s dive into uv by creating a new project. Projects are a way to manage your package dependencies for a coding project. We’ll start by installing the dependencies for the first radiology-oriented natural language processing (NLP) tutorial I presented in the RSNA Deep Learning Lab at the 2021 RSNA annual meeting."
  },
  {
    "objectID": "posts/install-pytorch.html#initializing-a-uv-project",
    "href": "posts/install-pytorch.html#initializing-a-uv-project",
    "title": "Managing Projects and Installing Pytorch with uv and light-the-torch",
    "section": "Initializing a uv Project",
    "text": "Initializing a uv Project\nIn the screenshot above, I’ve run the uv init command to initialize a project called “rnn-tutorial”. This command creates a new directory named rnn-tutorial. I then change directory or cd into the new directory and inspect it’s contents. You’ll notice that I’ve run two separate commands to inspect the files. The first command ls lists the contents of the directory. By default, this command excludes dotfiles, which are files named starting with a .. The second command la is an alias to the ls command with the -a flag that lists all files in a directory, including dotfiles.\nuv init rnn-tutorial\ncd rnn-tutorial\nls \nls -a \nRun these commands one-by-one in your terminal to follow along.\nThe first object you see listed in the directory is a subdirectory named .git. This is where all the files git uses for version control will be stored. Any files or patterns listed in the .gitignore file below .git will not be tracked by git. uv generates a default template that excludes files generated by Python (the ones you don’t want to track) and virtual environments of the type .venv, which are not in our project directory yet but will be created shortly.\nAfter those, a .python-verison file is created to specify the Python version used in the project. An example Python script named hello.py is included. A pyproject.toml file is generated that uses the TOML format to list information about the project, including dependencies, in machine-readable format. Finally, a blank README.md file is generated in which you should use Markdown formatting to include human-readable details about your project. If you commit your project to an online repository, such as GitHub or GitLab, then the README will be displayed on the repository page.\nRun the cat command followed by the file name in your terminal to inspect the contents of any of these files.\n\n\n\nuv Project Boilerplate"
  },
  {
    "objectID": "posts/install-pytorch.html#running-a-python-script",
    "href": "posts/install-pytorch.html#running-a-python-script",
    "title": "Managing Projects and Installing Pytorch with uv and light-the-torch",
    "section": "Running a Python Script",
    "text": "Running a Python Script\nLet’s run the example Python script hello.py by running the following command in your terminal.\nuv run hello.py\nThis command will use the default Python interpreter installed on your system to run the script. Since we haven’t yet created a virtual environment for this project, it will also do that and create a uv.lock file.\n\n\n\n\n\n\nCaution\n\n\n\nYou should NOT edit the uv.lock file directly. This will be managed by uv and contains precise dependencies for recreating your project on a different machine."
  },
  {
    "objectID": "posts/install-pytorch.html#installing-pytorch-and-fast.ai-with-uv-and-light-the-torch",
    "href": "posts/install-pytorch.html#installing-pytorch-and-fast.ai-with-uv-and-light-the-torch",
    "title": "Managing Projects and Installing Pytorch with uv and light-the-torch",
    "section": "Installing PyTorch and fast.ai with uv and light-the-torch",
    "text": "Installing PyTorch and fast.ai with uv and light-the-torch\nPyTorch is one of the most utilized software libraries for AI development. It also serves as the backbone for the fastai library. fast.ai is a multi-level framework developed by Jeremy Howard, Sylvain Gugger, and the fast.ai community that allows you to start with high-level commands with strong, empirical default settings that can train a deep learning model in just a few lines of code. As you learn more about fastai, you can progressively go deeper into the mid-level and low-level application programming interfaces (APIs) for more precise control over model development.\n\n\n\n\n\n\nTip\n\n\n\nThe fast.ai book is freely available for download via GitHub and also available in print form. I highly recommend the book and the associated courses (on the fast.ai website) for those who want to learn more about AI and deep learning.\n\n\nWe will first install PyTorch with a lightweight “wrapper” for pip called light-the-torch that simplifies the process of installing PyTorch and its dependencies to leverage the computing resources on your specific machine. You can read the documentation (or docs) for ltt here: ltt docs.\nWe’ll start by adding ltt to our project, then run uv sync to update our venv before using uv run to install PyTorch with ltt.\nuv add light-the-torch\nuv sync\nuv run ltt install torch\nAt the end of the output from the last command, you should see the successful installation of a number of packages, including a version of PyTorch, e.g. torch-2.9.1. Make a note of that version, then run the following command to ensure torch is added as a dependency to your project.\n# Replace the version number below with the version installed on your system\nuv add \"torch&gt;=2.9.1\"\nAnd, finally, we can now add fastai to our project.\nuv add fastai\nTo test that both the PyTorch and fast.ai libraries are properly installed on your system, create a new file in your project folder and copy-paste the following code into it. Assuming you have the VS Code editor installed, you can run the following command to open your project folder in Code: code .. Then create a new file called test.py, paste in the code, and save the file.\ndef main():\n    try:\n        import torch\n        print(f\"PyTorch is installed: {torch.__version__}\")\n        print(f\"MPS available: {torch.backends.mps.is_available()}\")\n    except ModuleNotFoundError:\n        print(\"PyTorch is NOT installed\")\n\n    try:\n        import fastai\n        print(f\"fastai is installed: {fastai.__version__}\")\n    except ModuleNotFoundError:\n        print(\"fastai is NOT installed\")\n\nif __name__ == \"__main__\":\n    main()\nNow, go back to your terminal (or open on in Code) and run the script with uv run. If you don’t get any errors and the package versions print out, then everything was successful!\n\n\n\n\n\n\nNote\n\n\n\nApple Metal Performance Shaders (MPS) is a backend for PyTorch available on Macs with an M-series GPU. If the check in the file above prints out “False”, then it’s likely that you have an older Mac and will only be able to utilize the CPU."
  },
  {
    "objectID": "posts/intro_cli.html",
    "href": "posts/intro_cli.html",
    "title": "Introduction to the Command Line Interface (CLI)",
    "section": "",
    "text": "Before you dig into Python and get to working on your algorithms, I highly recommend that you get familiar with the command line (variably referred to as the terminal or shell, though there is a distinct difference between each of these terms). If you’ve followed our Setup Guides, then you should already be somewhat familiar with the process of running some basic commands in the terminal. Either way, we’ll start with some basic shell terminology and functionality."
  },
  {
    "objectID": "posts/intro_cli.html#terminology",
    "href": "posts/intro_cli.html#terminology",
    "title": "Introduction to the Command Line Interface (CLI)",
    "section": "Terminology",
    "text": "Terminology\n\n\nTerminal\n\n\ntechnically, a terminal emulator, this is the command line interface or CLI - the window to the shell\n\n\nShell\n\n\nthe program that runs in the terminal allowing you to interact with and manipulate files\n\n\n\neach shell has its own functions, but there are many common functions and the language is mostly standardized (POSIX standard for Unix-based operating systems)\nCommonly used shells include:\n\nsh: a basic, lightweight shell\nbash: the Bourne-again shell, the most widely used shell\nzsh: the Z shell, added functionality and customization geared toward increasing productivity -&gt; my preferred shell\n\n\n\n\nScripts\n\n\nshort programs that are written in the shell language and run from the command line\n\n\nProcess\n\n\na running program. Every program running on your machine right now has at least one process associated with it.\n\n\nDotfiles\n\n\nfiles with names like .bashrc, .zshrc, or .tmux.conf that primarily serve as configuration files for the shell and its utilities. Most of these will live in your home folder /Users/walter on MacOS or /home/walter on Linux, both of which have the shortcut ~.\n\n\nDirectory\n\n\na.k.a. folder; contains files\n\n\nstdout\n\n\ni.e. standard output; what prints to your terminal when a command or script is executed"
  },
  {
    "objectID": "posts/intro_cli.html#purpose-of-the-shellcli",
    "href": "posts/intro_cli.html#purpose-of-the-shellcli",
    "title": "Introduction to the Command Line Interface (CLI)",
    "section": "Purpose of the shell/CLI",
    "text": "Purpose of the shell/CLI\nThe shell is a phenomenal tool for working with and managing files that are predominantly composed of text. These include *.txt, *.csv, and *.html files, in addition to Dotfiles and files without extensions, among many other file types. The shell is also really useful for managing processes and working on remote systems, such as servers.\n\n\n\nBash running in the Terminal app on MacOS\n\n\n\nDue to the ability to work directly and efficiently with text files, the shell is a critical tool for developers and data scientists. Basically, anyone regularly writing and running code could benefit from becoming more familiar with the shell. Given its ubiquity in the tech world, there are plenty of great tutorials out there that can help introduce you to the glories of the Command Line.\n\n\n\nZsh running in the iTerm app on MacOS\n\n\n\nThe tutorial I link to below is one of the more succinct introductions that should help you get started. However, it’s very easy to get stuck in the proverbial “Tutorial Hell”. So we won’t beat around the bush here.\nOne advantage of this tutorial is that you can run the shell code via the terminal emulator provided on the website. However, I recommend you try to get these scripts running in your own terminal.\nNote #1: As defined above, shell scripts are text files of shell code that can be run in the terminal with any output printed to the stdout of the shell. One way to create and run your own script is to run the following code. The structure or syntax of these commands is command --options arg1 arg2. You may not always use options or arguments (args) and some commands like chmod have special syntax for options.\nNote #2: in the below code, # precedes a “comment”, which is only intended to explain the code and is not executable code.\ntouch script.sh     # creates a new file in the current working directory\nls                  # list the files in the cwd, you should now see 'script.sh' among these\nchmod +x script.sh  # gives the current user execute permissions for the file\nnano script.sh      # open the file with the nano text editor (can use 'code' command for VS Code)\n\n# now you can copy and paste or transcribe the code from the tutorial into the file and save it\nbash script.sh      # runs the script\n\n\n\n\n\n\nNote\n\n\n\nLearn Shell Interactive Tutorial: Proceed to the linked page and complete each exercise under “Learn the Basics”.\n\n\nAfter you complete this tutorial, check back for the next Command Line post, where we’ll download some data for an intro to Machine Learning in Radiology and use shell functions & scripts to rename and move files into a more convenient directory structure before we move on to implement our first algorithm."
  },
  {
    "objectID": "posts/install-python.html",
    "href": "posts/install-python.html",
    "title": "Installing Python with uv",
    "section": "",
    "text": "In this post, we’ll cover a new-ish command line tool called uv that serves as both a Python package manager and project management tool. We’ll utilize this to install Python and set up an example coding project."
  },
  {
    "objectID": "posts/install-python.html#python",
    "href": "posts/install-python.html#python",
    "title": "Installing Python with uv",
    "section": "Python",
    "text": "Python\nPython is an interpreted programming language, sometimes referred to as a scripting language. It has been referred to as the lingua franca of AI and ML, so this is the language I use for all my interactive sessions and tutorials. The base Python installation comes with a number of packages for basic operations one might want to do when coding in Python. One of those packages is pip which is a package manager. This allows you to install other packages built for specific tasks. For example, you may want to install numpy so you can do numerical computing with arrays, or pandas so you can work with tabular data, or pytorch so you can do deep learning."
  },
  {
    "objectID": "posts/install-python.html#uv-for-package-management",
    "href": "posts/install-python.html#uv-for-package-management",
    "title": "Installing Python with uv",
    "section": "uv for Package Management",
    "text": "uv for Package Management\nuv is a faster package manager that also comes with other features to assist with project management. One such feature is virtual environment management. Having a virtual environment or “env” allows you to install packages specific to a project. Different projects (or tutorials) may require different versions of the same packages, or even a different version of Python itself. Installing those packages in an env helps you avoid conflicts between packages/versions in your base Python installation."
  },
  {
    "objectID": "posts/install-python.html#version-control",
    "href": "posts/install-python.html#version-control",
    "title": "Installing Python with uv",
    "section": "Version Control",
    "text": "Version Control\nAnother project management feature of uv is a tool called git for version control of coding projects. You can learn more about it at the link below. For now, you can think of it as a way to keep track of changes you make in your code, so that you can more easily revert to an earlier version if you make a change that breaks something. git also helps with managing collaborative development projects.\n\n\n\n\n\n\nMissing Semester: git (Version Control)\n\n\n\nTo learn more about git, check out this lecture from MIT’s The Missing Semester of your CS Education course: git (Version Control)."
  },
  {
    "objectID": "posts/install-python.html#installation",
    "href": "posts/install-python.html#installation",
    "title": "Installing Python with uv",
    "section": "Installation",
    "text": "Installation\nOpen up your terminal application, then copy and paste the following code into the terminal window to install uv on your system.\nwget -qO- https://astral.sh/uv/install.sh | sh\nAlternatively, if you’ve installed Homebrew on your system, you can copy and paste the following command into your terminal window to install uv.\nbrew install uv\nIt is wise to periodically update the libraries you have installed on your system. If you used the first command to install uv, then you may update the library with the following command.\nuv self update\nOtherwise, you may update uv with pip after you’ve installed Python.\n\n\n\n\n\n\nImportant\n\n\n\nYou must install Python before you run the following command.\npip install --upgrade uv"
  },
  {
    "objectID": "posts/install-python.html#installing-python-with-uv",
    "href": "posts/install-python.html#installing-python-with-uv",
    "title": "Installing Python with uv",
    "section": "Installing Python with uv",
    "text": "Installing Python with uv\nNow that uv is installed, you are able to install a base version of Python to use regardless of whether you’ve setup a virtual environment for a given project.\nIf you have a recent model of Mac, then Python is likely already installed on your system. Run the following command in your terminal to see if Python is already installed and, if so, which version.\npython --version\nYou can use uv to manage previously installed Python versions. Run the following command if you’d like to use and update the existing version of Python.\nuv python install --reinstall\nIf you do not yet have Python installed, run the following command to install it with uv.\nuv python install\nWith that, you should have a working base installation of Python, so any time you create a virtual environment with uv, it will include that base version by default unless you specify a version of Python to use.\nIn the next post, we’ll setup a virtual environment and install Pytorch – one of the leading libraries for deep learning."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Radiology ML Tutorials Blog",
    "section": "",
    "text": "Welcome!\nI set up this website to complement the interactive artificial intelligence (AI), deep learning (DL), and machine learning (ML) tutorials I’ve conducted over the years with RSNA, SIIM, ASNR, and other radiology organizations. Over time, I’m hoping this will evolve into a regular resource for individuals looking to learn more about radiology AI with an emphasis on hands-on learning with (some) code.\nFor those just beginning their journey into radiology AI and/or coding with Python, I have included some recommended articles, courses, and tutorials in the Resources tab above.\nMany of the posts below will be interactive tutorials. For the best experience, you should have a Google account, so you can open the notebooks in Google Colab and run/interact with the demonstrations.\nIf you’re interested in setting up a development enviroment on your own machine, so you can experiment with AI on local data, then please refer to the basic and advanced Local Dev Setup guides linked in the top menu bar.\n\n\nPosts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nIntroduction to the Command Line Interface (CLI)\n\n\n\n\n\n\n\nbasics\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2024\n\n\nWalter Wiggins\n\n\n\n\n\n\n  \n\n\n\n\nInstalling Python with uv\n\n\n\n\n\n\n\nbasics\n\n\nsetup\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2025\n\n\nWalter Wiggins\n\n\n\n\n\n\n  \n\n\n\n\nManaging Projects and Installing Pytorch with uv and light-the-torch\n\n\n\n\n\n\n\nbasics\n\n\nsetup\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2025\n\n\nWalter Wiggins\n\n\n\n\n\n\n  \n\n\n\n\nRNNs Tutorial from RSNA 2021 Deep Learning Lab\n\n\n\n\n\n\n\n\n\n\n\n\nDec 28, 2025\n\n\nWalter Wiggins\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "advanced-setup.html",
    "href": "advanced-setup.html",
    "title": "Advanced Setup",
    "section": "",
    "text": "These are optional steps for those interested in taking advantage of some advanced tools for the CLI. I find these tools very useful, but it takes a fair amount of time to get everything installed and running. There’s also the possibility that you’ll spend a considerable amount of time tweaking things to your liking…\nAlso, if you’re a MacOS user and haven’t yet installed the Xcode Developer Command Line Tools, return to the Basic Setup Guide and do this first."
  },
  {
    "objectID": "advanced-setup.html#zsh-the-z-shell",
    "href": "advanced-setup.html#zsh-the-z-shell",
    "title": "Advanced Setup",
    "section": "Zsh: the Z shell",
    "text": "Zsh: the Z shell\nZsh now comes standard on most recent versions of MacOS, but traditionally the default shell on MacOS and most Linux distributions has been bash. zsh is thus considered to be an alternative shell. It offers some really nice features like improved tab-completion (probably the most routinely useful one), syntax highlighting, and extended “globbing” - I’ll explain the last one later in a post.\nLet’s start by installing iTerm2, Homebrew, Oh My Zsh and Powerline fonts to make sure everything runs smoothly once we actually start to use the shell.\n\nInstall iTerm2 from the website. iTerm2 is a more customizable terminal emulator for MacOS.\nOpen iTerm2. You can use Spotlight {CMD+SPACE}, then type iTerm.\nInstall Homebrew by copying and pasting the command below into your terminal. Click the link to learn more about this package manager for MacOS.\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\""
  },
  {
    "objectID": "advanced-setup.html#oh-my-zsh",
    "href": "advanced-setup.html#oh-my-zsh",
    "title": "Advanced Setup",
    "section": "Oh My Zsh",
    "text": "Oh My Zsh\nOh My Zsh is a collection of themes for taking full advantage of Zsh and “beautifying your shell”. By installing Xcode Command Line Tools, you now have git, a version management command line tool that you’ll be using a lot, if you continue to use this site.\nWe’ll install Oh-My-Zsh from Robby Russell’s GitHub repository (or ‘repo’).\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"\nOnce this is is done, you’ll be running your fancy new Zsh with Oh My Zsh. The default theme is robbyrussell. We’ll change that shortly, though you can certainly keep it if you like it."
  },
  {
    "objectID": "advanced-setup.html#powerline-fonts",
    "href": "advanced-setup.html#powerline-fonts",
    "title": "Advanced Setup",
    "section": "Powerline Fonts",
    "text": "Powerline Fonts\nMany Oh My Zsh themes require a Powerline font to be rendered properly in the terminal. So, naturally, we’ll install those here to keep things running smoothly as we proceed. (Note: In the code block below, the # indicates a comment, not a command.) Back to our new friend git…\n# clone the Powerline fonts repo to your machine\ngit clone https://github.com/powerline/fonts.git --depth=1\n\n# run the install script\ncd fonts\n./install.sh\n\n# clean up the files you no longer need\ncd ..\nrm -rf fonts\nNow, head to “Preferences” in iTerm2 by pressing {CMD + ,}. Then go to “Profiles &gt; Text” to set the “Regular” and non-ASCII fonts to one of your newly installed Powerline fonts (I recommend SourceCodePro for starters). I also recommend switching your color palette to “Solarized”, as it plays nicely with Oh My Zsh."
  },
  {
    "objectID": "advanced-setup.html#configuring-zsh-and-oh-my-zsh-with-nano",
    "href": "advanced-setup.html#configuring-zsh-and-oh-my-zsh-with-nano",
    "title": "Advanced Setup",
    "section": "Configuring Zsh and Oh My Zsh with Nano",
    "text": "Configuring Zsh and Oh My Zsh with Nano\nMost commonly, people configure Zsh in their .zshrc file. Installing OMZ will prepopulate yours with some useful stuff. Running ls in ~ won’t show you “dotfiles” (e.g. .zshrc) by default. For that, you should run ls -a (or la - an “alias” or shortcut for the preceding command). You should see something similar to what I’ve shown below, though everyone will probably have many other files and folders that get printed to the standard output (or “stdout”) when they run this command.\n$ ls -a\n-rw-------  1 walter walter  496 Oct  6 22:36 .bash_history\n-rw-r--r--  1 walter walter  220 Oct  6 22:50 .bashrc\ndrwxr-xr-x 11 walter walter 4.0K Oct  6 22:46 .oh-my-zsh\n-rw-------  1 walter walter  56K Nov 15 11:35 .zsh_history\n-rw-r--r--  1 walter walter 3.8K Nov 15 11:25 .zshrc\n# ... with other files mixed in\nNow, on to configuring Zsh in .zshrc. The first thing you should do is decide which text editor you want to use. Your options are a graphical text editor or one of the built-in text editors that run in the shell itself. For simplicity, we’ll go with nano - one of the built-in options. It’s far easier to use than vi or vim and simpler than opening a separate window every time you want to make a minor change to your config. Entering nano ~/.zshrc will launch nano with your config file in the editor. At the bottom of the window, you should see the commands available to you (FYI: ^G is shorthand for {CTRL + G}). After you make changes, you will need to press ^O (letter O, not number) to “write out” (i.e. save) your changes before you ^X (exit). Also, you’ll be navigating with the keyboard as your mouse won’t work.\nEverything you see in your .zshrc has been added by OMZ. The first thing we’ll do is change the theme. Navigate to the line that says ZSH_THEME=\"robbyrussell\". We’re going to change the theme to “agnoster”, so delete “robbyrussell” and replace it with “agnoster” such that the line now looks like this ZSH_THEME=\"agnoster\". Then ^O and ^X. Now you’ve saved your changes, exited nano and are back in Zsh…but everything looks the same.\nTo see the effects of your changes to the config, you’ll have to reload your shell. The command for this is source ~/.zshrc. Over time, you may find yourself tweaking your shell config regularly, adding or changing different aliases, etc. Thus, I recommend you go ahead and add the following lines to your .zshrc under # Aliases. You may wish to delete the existing ones.\nalias zshconfig=\"nano ~/.zshrc\"\nalias reload=\"source ~/.zshrc\"\nYou will have to reload your shell profile with the full command before you can use your new aliases. But after you do that, you’ll be able to edit your config in nano simply by entering zshconfig, then reload when you’re done.\nIf you are using the VS Code editor, you can replace the nano command with the code command."
  },
  {
    "objectID": "notebooks/RNNs_tutorial.html",
    "href": "notebooks/RNNs_tutorial.html",
    "title": "RSNA 2021: Deep Learning Lab",
    "section": "",
    "text": "In this demonstration, we will utilize techniques of natural language processing (NLP) to train a classifier, which will analyze the text of radiology reports for chest radiographs to predict whether a report is normal or abnormal.\n\n\nWe will utilize the fast.ai v2 library, written primarily by Jeremy Howard and Sylvain Gugger (with help from many others). It is written in the Python programming language and built on top of the PyTorch deep learning library.\nThe demonstration in this notebook relies heavily on examples from the fast.ai book, Deep Learning for Coders with fastai and PyTorch: AI Applications without a PhD by Jeremy Howard and Sylvain Gugger, which was written entirely in Jupyter notebooks, which are freely available for download on GitHub. A print copy of the book can be purchased from Amazon.\n\n\n\nThe data is obtained from the National Library of Medicine’s Open-i service. We utilize the radiology reports from the Indiana University Chest X-ray Dataset for this demonstration.\n\nReference: Demner-Fushman D, Kohli MD, Rosenman MB, Shooshan SE, Rodriguez L, Antani S, Thoma GR, McDonald CJ. Preparing a collection of radiology examinations for distribution and retrieval. J Am Med Inform Assoc. 2016 Mar;23(2):304-10. doi: 10.1093/jamia/ocv080. Epub 2015 Jul 1.\n\n\n\n\n\nWalter F. Wiggins, MD, PhD - Duke University Hospital, Durham, NC, USA\nFelipe Kitamura, MD - UNIFESP, Sao Paulo, Brasil\nIgor Santos, MD - UNIFESP, Sao Paulo, Brasil\nLuciano M. Prevedello, MD, MPH - Ohio State University, Columbus, OH, USA"
  },
  {
    "objectID": "notebooks/RNNs_tutorial.html#basics-of-information-extraction-from-radiology-reports",
    "href": "notebooks/RNNs_tutorial.html#basics-of-information-extraction-from-radiology-reports",
    "title": "RSNA 2021: Deep Learning Lab",
    "section": "",
    "text": "In this demonstration, we will utilize techniques of natural language processing (NLP) to train a classifier, which will analyze the text of radiology reports for chest radiographs to predict whether a report is normal or abnormal.\n\n\nWe will utilize the fast.ai v2 library, written primarily by Jeremy Howard and Sylvain Gugger (with help from many others). It is written in the Python programming language and built on top of the PyTorch deep learning library.\nThe demonstration in this notebook relies heavily on examples from the fast.ai book, Deep Learning for Coders with fastai and PyTorch: AI Applications without a PhD by Jeremy Howard and Sylvain Gugger, which was written entirely in Jupyter notebooks, which are freely available for download on GitHub. A print copy of the book can be purchased from Amazon.\n\n\n\nThe data is obtained from the National Library of Medicine’s Open-i service. We utilize the radiology reports from the Indiana University Chest X-ray Dataset for this demonstration.\n\nReference: Demner-Fushman D, Kohli MD, Rosenman MB, Shooshan SE, Rodriguez L, Antani S, Thoma GR, McDonald CJ. Preparing a collection of radiology examinations for distribution and retrieval. J Am Med Inform Assoc. 2016 Mar;23(2):304-10. doi: 10.1093/jamia/ocv080. Epub 2015 Jul 1.\n\n\n\n\n\nWalter F. Wiggins, MD, PhD - Duke University Hospital, Durham, NC, USA\nFelipe Kitamura, MD - UNIFESP, Sao Paulo, Brasil\nIgor Santos, MD - UNIFESP, Sao Paulo, Brasil\nLuciano M. Prevedello, MD, MPH - Ohio State University, Columbus, OH, USA"
  },
  {
    "objectID": "notebooks/RNNs_tutorial.html#interactive-participation",
    "href": "notebooks/RNNs_tutorial.html#interactive-participation",
    "title": "RSNA 2021: Deep Learning Lab",
    "section": "Interactive Participation",
    "text": "Interactive Participation\nThis notebook is designed for interactive participation.\nPlease refer to the previous posts on installing Python and installing PyTorch and fast.ai with uv to ensure your environment is set up properly. You’ll also need to run the following commands to add two additional packages to your uv project.\nuv add xmltodict\nuv add tensorboard\nYou can run the code cells in order, and modify parameters as desired to see how they affect the results.\nIf you are using Google Colab, please use the link below to open a compatible version in Colab. You can save a copy of the notebook to your own Google Drive by selecting “File” -&gt; “Save a copy in Drive”.\nOpen in Colab"
  },
  {
    "objectID": "notebooks/RNNs_tutorial.html#tokenization",
    "href": "notebooks/RNNs_tutorial.html#tokenization",
    "title": "RSNA 2021: Deep Learning Lab",
    "section": "Tokenization",
    "text": "Tokenization\nTokenization is the process by which we split text into chunks or tokens. An individual token could be a sentence or phrase, a word, a part of a word, or a single character. Most often, we choose to tokenize at the word or sub-word level.\nEach token is assigned a sequential integer value and the collection of token-integer pairs is called our vocabulary.\n\n#@title **Word Tokenization**\n\nspacy = WordTokenizer()\ntext = train_val.iloc[0]['full-text']\nprint('Original text:')\nprint(text)\nprint()\ntkns = first(spacy([text]))\nprint('After word tokenization:')\nprint(coll_repr(tkns))\n\nOriginal text:\nThe XXXX examination consists of frontal and lateral radiographs of the chest. The cardiac silhouette is not enlarged. There has been apparent interval increase in low density convexity at the left cardiophrenic XXXX. Calcified granuloma is again seen in the right upper lobe. There is no consolidation, pleural effusion or pneumothorax. Increased size of density in the left cardiophrenic XXXX. Primary differential considerations include increased size of prominent epicardial fat, pericardial mass, pleural mass or cardiac aneurysm. CT chest with contrast is recommended. These findings and recommendations were discussed XXXX. XXXX by Dr. XXXX XXXX telephone at XXXX p.m. XXXX/XXXX. Dr. XXXX&lt;XXXX&gt;technologist receipt of the results.\n\nAfter word tokenization:\n['The', 'XXXX', 'examination', 'consists', 'of', 'frontal', 'and', 'lateral', 'radiographs', 'of', 'the', 'chest', '.', 'The', 'cardiac', 'silhouette', 'is', 'not', 'enlarged', '.', 'There', 'has', 'been', 'apparent', 'interval', 'increase', 'in', 'low', 'density', 'convexity', 'at', 'the', 'left', 'cardiophrenic', 'XXXX', '.', 'Calcified', 'granuloma', 'is', 'again', 'seen', 'in', 'the', 'right', 'upper', 'lobe', '.', 'There', 'is', 'no', 'consolidation', ',', 'pleural', 'effusion', 'or', 'pneumothorax', '.', 'Increased', 'size', 'of', 'density', 'in', 'the', 'left', 'cardiophrenic', 'XXXX', '.', 'Primary', 'differential', 'considerations', 'include', 'increased', 'size', 'of', 'prominent', 'epicardial', 'fat', ',', 'pericardial', 'mass', ',', 'pleural', 'mass', 'or', 'cardiac', 'aneurysm', '.', 'CT', 'chest', 'with', 'contrast', 'is', 'recommended', '.', 'These', 'findings', 'and', 'recommendations', 'were', 'discussed', 'XXXX', '.', 'XXXX', 'by', 'Dr.', 'XXXX', 'XXXX', 'telephone', 'at', 'XXXX', 'p.m.', 'XXXX', '/', 'XXXX', '.', 'Dr.', 'XXXX', '&lt;', 'XXXX', '&gt;', 'technologist', 'receipt', 'of', 'the', 'results', '.']"
  },
  {
    "objectID": "notebooks/RNNs_tutorial.html#added-features-from-fast.ai",
    "href": "notebooks/RNNs_tutorial.html#added-features-from-fast.ai",
    "title": "RSNA 2021: Deep Learning Lab",
    "section": "Added features from fast.ai",
    "text": "Added features from fast.ai\nYou’ll notice some odd-appearing tokens in the output from the next cell. These are special tokens that indicate certain things about the text. - 'xxbos' indicates the beginning of the text stream - 'xxmaj' indicates that the following character was capitalized before fast.ai lowered it - 'xxrep' followed by '4', 'x' means that 'x' is repeated 4 times…the XXXX in the reports is a product of the anonymization process that the team who generated the dataset used\nThese special tokens enrich the data while reducing the vocab by eliminating redundant upper and lower case variants of individual words.\n\n#@title **fast.ai Tokenization**\n\ntkn = Tokenizer(spacy)\ntoks = tkn(text)\nprint(coll_repr(toks, 15))\n\n(#164) ['xxbos', 'xxmaj', 'the', 'xxrep', '4', 'x', 'examination', 'consists', 'of', 'frontal', 'and', 'lateral', 'radiographs', 'of', 'the'...]"
  },
  {
    "objectID": "notebooks/RNNs_tutorial.html#numericalization",
    "href": "notebooks/RNNs_tutorial.html#numericalization",
    "title": "RSNA 2021: Deep Learning Lab",
    "section": "Numericalization",
    "text": "Numericalization\nAfter converting text to tokens, the next step is to convert each unique token to a number.\nFor language modeling, we should do this on all of the text that we might use for training and validation. So we’ll carry this out on the combined Findings and Impression for each of our reports and tokenize the text to define our vocabulary. Each token in the vocabulary will be identified by a unique number.\n\ntxts = L(train_val['full-text'].to_list())\ntoks = txts.map(tkn)\nnum = Numericalize()\nnum.setup(toks)\ncoll_repr(num.vocab, 20)\n\n\"(#1192) ['xxunk', 'xxpad', 'xxbos', 'xxeos', 'xxfld', 'xxrep', 'xxwrep', 'xxup', 'xxmaj', '.', 'no', 'the', 'is', 'are', 'normal', ',', '4', 'x', 'of', 'and'...]\"\n\n\nAs you can see above, we have 1,192 tokens in our vocabulary. The special tokens from fast.ai appear first, followed by every other token in order of decreasing frequency. By default, fast.ai only includes tokens that appear at least 3 times in the corpus (collection of texts).\n\nnums = num(toks[0][:14])\nprint(nums)\nprint(' '.join(num.vocab[i] for i in nums))\n\nTensorText([  2,   8,  11,   5,  16,  17, 162, 314,  18, 178,  19,  89, 261,\n             18])\nxxbos xxmaj the xxrep 4 x examination consists of frontal and lateral radiographs of\n\n\nHere, we see a subset of the numericalized tokens (top) with their corresponding word tokens (bottom) from the first report in our corpus."
  },
  {
    "objectID": "notebooks/RNNs_tutorial.html#embedding",
    "href": "notebooks/RNNs_tutorial.html#embedding",
    "title": "RSNA 2021: Deep Learning Lab",
    "section": "Embedding",
    "text": "Embedding\nWhile these integers map one-to-one onto our tokens, their numeric value is otherwise meaningless. To embed more information into the numeric representation of our tokens, we employ a process called language modeling. We can either use a pre-trained language model for this embedding or we can fine-tune a language model to better model our “radiology language”. This latter process is called transfer learning."
  },
  {
    "objectID": "notebooks/RNNs_tutorial.html#language-model-transfer-learning",
    "href": "notebooks/RNNs_tutorial.html#language-model-transfer-learning",
    "title": "RSNA 2021: Deep Learning Lab",
    "section": "Language Model Transfer Learning",
    "text": "Language Model Transfer Learning\n\nGiven a string of tokenized text, a language model is trained to predict the next token.\n\nBy default, fastai.text uses a language model pre-trained on the WikiText-103 corpus, which is a frequently used benchmark for NLP models, similar to ImageNet for image classification models.\nIn their 2018 ACL paper “Universal Language Model Fine-Tuning for Text Classification”, Jeremy Howard and Sebastian Ruder proposed the ULM-FiT approach of fine-tuning the language model on the corpus on which a second text classification model is trained. In the paper, they demonstrated state-of-the-art performance on a commonly used text classification example using this method.\nHere, we will fine-tune the language model on our radiology report text in two stages.\n\n\n\n\n\n\nNote\n\n\n\nThis portion of the notebook takes between 3 and 10 minutes to run.\n\n\n\ndls_lm = TextDataLoaders.from_df(train_val, valid_pct=0.3, seed=42, text_col='full-text', is_lm=True)\nlearn_lm = language_model_learner(dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()]).to_fp16()\n\n# Train the final layers of the network\nlearn_lm.fit_one_cycle(1, 2e-2)\nprint(\"\\nPart 1/2 complete.\\n\")\n\n# Unfreeze and train some more\nlearn_lm.unfreeze()\nlearn_lm.fit_one_cycle(10, 2e-3)\nprint(\"\\nPart 2/2 complete.\\n\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n3.247110\n2.256087\n0.519232\n9.545664\n00:06\n\n\n\n\n\n\nPart 1/2 complete.\n\n\nPart 2/2 complete.\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n2.391538\n2.023878\n0.559515\n7.567616\n00:06\n\n\n1\n2.201109\n1.808918\n0.594847\n6.103842\n00:07\n\n\n2\n2.012767\n1.621929\n0.635380\n5.062846\n00:06\n\n\n3\n1.852338\n1.532954\n0.657017\n4.631841\n00:06\n\n\n4\n1.725044\n1.483582\n0.665437\n4.408710\n00:07\n\n\n5\n1.609891\n1.435381\n0.675197\n4.201245\n00:07\n\n\n6\n1.514347\n1.408834\n0.681890\n4.091181\n00:07\n\n\n7\n1.443644\n1.396428\n0.683933\n4.040739\n00:07\n\n\n8\n1.378929\n1.390368\n0.685186\n4.016330\n00:06\n\n\n9\n1.340931\n1.389609\n0.685622\n4.013279\n00:07\n\n\n\n\n\nWe need to save this model state in a special format in order to use it for our Embedding layer in the text classification model.\nWe also want to use the same vocabulary that we used for the ULM-FiT procedure in our classification model.\n\ntorch.save(learn_lm.dls.vocab, 'vocab.pkl')\nlearn_lm.save_encoder('fine-tuned-enc')\nprint('Saved.')\n\nSaved."
  },
  {
    "objectID": "notebooks/RNNs_tutorial.html#iterative-training-process",
    "href": "notebooks/RNNs_tutorial.html#iterative-training-process",
    "title": "RSNA 2021: Deep Learning Lab",
    "section": "Iterative Training Process",
    "text": "Iterative Training Process\nSo that we don’t propagate too much error through the network during training, we will train the final layers first, then iteratively unfreeze (i.e. make trainable) a few preceding layers and train a little more before repeating this process.\n\nlearn_cls = text_classifier_learner(dls_cls, AWD_LSTM, drop_mult=0.5, metrics=accuracy).to_fp16()\nif fine_tuned_LM: learn_cls.load_encoder('fine-tuned-enc')\nlearn_cls.fit_one_cycle(1, 2e-2)\nprint(\"\\nPart 1/4 complete.\\n\")\n\n# Unfreezing and training a little more...\nlearn_cls.freeze_to(-2)\nlearn_cls.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\nprint(\"\\nPart 2/4 complete.\\n\")\n\n# Unfreezing and training a little more, again...\nlearn_cls.freeze_to(-3)\nlearn_cls.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\nprint(\"\\nPart 3/4 complete.\\n\")\n\n# Now let's unfreeze the whole model and finish training.\nlearn_cls.unfreeze()\nlearn_cls.fit_one_cycle(1, slice(1e-3/(2.6**4),1e-3))\nprint(\"\\nPart 4/4 complete.\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.524007\n0.356475\n0.926587\n00:04\n\n\n\n\n\n\nPart 1/4 complete.\n\n\nPart 2/4 complete.\n\n\nPart 3/4 complete.\n\n\nPart 4/4 complete.\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.485159\n0.249356\n0.937500\n00:04\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.383564\n0.162610\n0.958333\n00:05\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.300795\n0.136703\n0.960317\n00:07\n\n\n\n\n\n\n# Export fine-tuned classifier model\n\nlearn_cls.export('full-text')"
  },
  {
    "objectID": "notebooks/RNNs_tutorial.html#tensorboard",
    "href": "notebooks/RNNs_tutorial.html#tensorboard",
    "title": "RSNA 2021: Deep Learning Lab",
    "section": "Tensorboard",
    "text": "Tensorboard\nRun the following cell, then hit the refresh button in the top right menu of the Tensorboard iframe that appears.\nNext, look along the left hand menu for T-SNE. Click it and let it run for ~1,000 iterations. Then hit the “stop” button.\nNow, explore the embedding space to see how radiological terms cluster in the embedding space of a language model fine-tuned on CXR reports.\n\n# Load tensorboard extension and packages\n%load_ext tensorboard\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\n\nlearn = load_learner(\"full-text\")\n\nm = learn.model[0]\nif hasattr(m, \"module\"):  # DataParallel/DDP\n    m = m.module\n\n# AWD_LSTM encoder weights (token embedding matrix)\nW = m.encoder.weight.detach().cpu()          # shape: [vocab_sz, emb_dim]  [oai_citation:1‡cedrickchee.gitbook.io](https://cedrickchee.gitbook.io/knowledge/courses/fast.ai/deep-learning-part-2-cutting-edge-deep-learning-for-coders/2018-edition/lesson-10-transfer-learning-nlp?utm_source=chatgpt.com)\nW = F.normalize(W, dim=1)                       # optional\n\nvocab = list(learn.dls.vocab[0])                # len == vocab_sz\n\nlog_dir = \"runs/embeddings_awdlstm\"\nwriter = SummaryWriter(log_dir)\nwriter.add_embedding(W, metadata=vocab, tag=\"word_embeddings\")\nwriter.close()\n\n%tensorboard --logdir runs"
  },
  {
    "objectID": "notebooks/RNNs_tutorial.html#testing-the-model",
    "href": "notebooks/RNNs_tutorial.html#testing-the-model",
    "title": "RSNA 2021: Deep Learning Lab",
    "section": "Testing the model",
    "text": "Testing the model\nNow that our model is trained, let’s test it on our held-out test dataset.\nWe will run our report classification model on the held-out test dataset. A report will be generated with the following information: - precision: a.k.a. positive predictive value = true_positives / all_predicted_positives - recall: a.k.a. sensitivity = true_positives / all_actual_positives - f1-score: the harmonic mean of precision and recall - accuracy: number_correct / number_reports\n\nNote: Support = number of reports used to derive the result in that row\n\n\ntest_items = test_df[[training_target, 'label']].copy()\ntest_items.columns = ['text', 'label']\ndl_test = learn_cls.dls.test_dl(test_items, with_labels=True)\ninterp = ClassificationInterpretation.from_learner(learn_cls, dl=dl_test)\ninterp.print_classification_report()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n    abnormal       0.98      0.95      0.96       385\n      normal       0.91      0.96      0.93       209\n\n    accuracy                           0.95       594\n   macro avg       0.94      0.95      0.95       594\nweighted avg       0.95      0.95      0.95       594"
  },
  {
    "objectID": "notebooks/RNNs_tutorial.html#the-confusion-matrix",
    "href": "notebooks/RNNs_tutorial.html#the-confusion-matrix",
    "title": "RSNA 2021: Deep Learning Lab",
    "section": "The confusion matrix",
    "text": "The confusion matrix\nThe confusion matrix is a graphical representation of how the model performed. In our case, it is a 2x2 table comparing model predictions to the actual label for each class (i.e. “normal” and “abnormal”).\n\ninterp.plot_confusion_matrix()"
  },
  {
    "objectID": "notebooks/RNNs_tutorial.html#analyzing-the-failures-or-top-losses",
    "href": "notebooks/RNNs_tutorial.html#analyzing-the-failures-or-top-losses",
    "title": "RSNA 2021: Deep Learning Lab",
    "section": "Analyzing the failures or “top losses”",
    "text": "Analyzing the failures or “top losses”\nThe loss function gives us a numerical value telling us how far off the model’s prediction is from the actual class. The higher the loss, the more confused our model is.\nAnalyzing the top losses can help you understand why your model fails when it fails. Here, we plot the top 3 losses for our test dataset.\n\ninterp.plot_top_losses(3)\n\n\n\n\n\n\n\n\n\n\n\n\ninput\ntarget\npredicted\nprobability\nloss\n\n\n\n\n0\nxxbos xxmaj the heart size and pulmonary vascularity appear within normal limits . xxmaj lungs are free of focal airspace disease . xxmaj no pleural effusion or pneumothorax is seen . xxmaj no discrete nodules or adenopathy are noted . xxmaj degenerative changes are present in the spine . xxmaj no evidence of active disease .\nnormal\nabnormal\n0.9675442576408386\n4.6323370933532715\n\n\n1\nxxbos xxmaj the cardiomediastinal silhouette is normal in size and contour . xxmaj no focal consolidation , pneumothorax or large pleural effusion . xxmaj normal xxrep 4 x . xxmaj negative for acute abnormality .\nabnormal\nnormal\n0.9576185345649719\n3.4278769493103027\n\n\n2\nxxbos xxmaj heart xxrep 4 x , mediastinum , xxrep 4 x , bony structures and lung xxrep 4 x are unremarkable . xxmaj no radiographic evidence of acute cardiopulmonary disease\nabnormal\nnormal\n0.9902680516242981\n3.1610445976257324"
  }
]