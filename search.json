[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I‚Äôm a neuroradiologist who likes to code. By day, I split my time between (1) taking care of patients by interpreting diagnostic imaging of the brain, head, neck, and spine, as well as doing image-guided, minimally invasive procedures; and (2) working hard to deploy and monitor AI tools for radiology.\nEducation around AI and machine learning is vital for radiologists. While we don‚Äôt need to understand everything about how these tools work, I think it helps to understand a little about how ‚Äúthe sausage is made‚Äù in order to use these tools effectively. Just like diagnostic imaging modalities, you should have a basic understanding of AI‚Äôs uses and limitations, as well as some common ways it can fail.\nThis blog is dedicated to interactive tutorials covering various topics in AI & ML for medical imaging and natural language processing (NLP) or text analysis.\nI hope you enjoy!\nCheers, Walter"
  },
  {
    "objectID": "posts/Image_Classification_Tutorial.html#introduction",
    "href": "posts/Image_Classification_Tutorial.html#introduction",
    "title": "Image Classification for Beginners",
    "section": "Introduction",
    "text": "Introduction\nIn this demonstration, we will utilize techniques of computer vision, including deep convolutional neural networks (CNNs), to train an image classifier model capable of classifying radiographs as either chest or abdominal.\n\nCode\nWe will utilize the fast.ai v2 library, written primarily by Jeremy Howard and Sylvain Gugger (with help from many others). It is written in the Python programming language and built on top of the PyTorch deep learning library.\nThe demonstration in this notebook relies heavily on examples from the fast.ai book, Deep Learning for Coders with fastai and PyTorch: AI Applications without a PhD by Jeremy Howard and Sylvain Gugger, which was written entirely in Jupyter notebooks, which are freely available for download on GitHub. A print copy of the book can be purchased from Amazon.\n\n\nData\nThis work is adapted from ‚ÄúHello World Deep Learning in Medical Imaging {% fn 1 %}‚Äù. The chest and abdominal radiographs were obtained from Paras Lakhani‚Äôs GitHub repository.\n{{ ‚ÄúReference: Lakhani P, Gray DL, Pett CR, Nagy P, Shih G. Hello World Deep Learning in Medical Imaging. J Digit Imaging. 2018 Jun; 31(3):283-289. Published online 2018 May 3. doi: 10.1007/s10278-018-0779-6‚Äù | fndetail: 1 }}\n\n\nDevelopers\n\nWalter F. Wiggins, MD, PhD - Duke University Hospital, Durham, NC, USA\nKirti Magudia, MD, PhD, - University of California, San Francisco, CA, USA\nM. Travis Caton, MD, PhD - University of California, San Francisco, CA, USA\n\n\n\nAcknowledgements\nOther versions of this notebook implemented on the Kaggle Notebooks platform were presented at the 2019 Society for Imaging Informatics in Medicine (SIIM) Annual Meeting and for the American College of Radiology (ACR) Residents & Fellows Section (RFS) AI Journal Club.\nWe would also like to acknowledge the following individuals for inspiring our transition to the Google Colab platform with their excellent notebook from the 2019 RSNA AI Refresher Course: - Luciano M. Prevedello, MD, PhD - Felipe C. Kitamura, MD, MSc - Igor Santos, MD - Ian Pan, MD"
  },
  {
    "objectID": "posts/Image_Classification_Tutorial.html#system-setup-downloading-the-data",
    "href": "posts/Image_Classification_Tutorial.html#system-setup-downloading-the-data",
    "title": "Image Classification for Beginners",
    "section": "System Setup & Downloading the Data",
    "text": "System Setup & Downloading the Data\n\nImportant: Save a copy of this notebook in your Google Drive folder by selecting Save a Copy in Drive from the File menu in the top left corner of this page. This will allow you to modify the cells and save your results.\n\n\n\nWarning: Make sure you have the runtime type set to ‚ÄúGPU‚Äù. See GIF below.\n\n\n\n\n\nSet Runtime to GPU\n\n\n\nSetting up the runtime environment‚Ä¶\nRunning the following cell in Colab will install the necessary libraries, download the data and restart the session.\n\nWarning: This will generate an error message, which we can safely ignore üòâ.\n\n::: {.cell _uuid=‚Äò0c1e51dbcbe223c29555c5188b0df55b10ed8b06‚Äô cellView=‚Äòform‚Äô}\nimport os\n\n!pip install fastai==2.1.4 &gt;/dev/null\n!pip install fastcore==1.3.1 &gt;/dev/null\n\n# **Downloading the data...**\n\n!wget -q https://github.com/wfwiggins/RSNA-Image-AI-2020/blob/master/data.zip?raw=true\n!mkdir -p data\n!unzip -o data.zip?raw=true -d data &gt;/dev/null\n!rm data.zip?raw=true\n\nos.kill(os.getpid(), 9)\n:::"
  },
  {
    "objectID": "posts/Image_Classification_Tutorial.html#exploring-the-data",
    "href": "posts/Image_Classification_Tutorial.html#exploring-the-data",
    "title": "Image Classification for Beginners",
    "section": "Exploring the Data",
    "text": "Exploring the Data\nLet‚Äôs take a look at the directory structure and contents, then create some variables to help us as we proceed.\n\n#collapse\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (12, 12)\n\n::: {.cell _uuid=‚Äò4cfcb87c72a542aefce13f8453097c0d1eb0c7b9‚Äô cellView=‚Äòform‚Äô outputId=‚Äòc18af866-27cb-4f1b-b4a9-7e76ce8456e2‚Äô execution_count=2}\nfrom fastai.basics import *\nfrom fastai.vision.all import *\n\n# Set path variable to the directory where the data is located\npath = Path('/content/data')\n\n# Command line \"magic\" command to show directory contents\n!ls {path}/**/*\n\n/content/data/test/abd:\nabd_test.png\n\n/content/data/test/chest:\nchest_test.png\n\n/content/data/train/abd:\nabd0.png   abd14.png  abd19.png  abd23.png  abd28.png  abd3.png  abd8.png\nabd10.png  abd15.png  abd1.png   abd24.png  abd29.png  abd4.png  abd9.png\nabd11.png  abd16.png  abd20.png  abd25.png  abd2.png   abd5.png\nabd12.png  abd17.png  abd21.png  abd26.png  abd30.png  abd6.png\nabd13.png  abd18.png  abd22.png  abd27.png  abd31.png  abd7.png\n\n/content/data/train/chest:\nchst33.png  chst39.png  chst45.png  chst51.png  chst57.png  chst63.png\nchst34.png  chst40.png  chst46.png  chst52.png  chst58.png  chst64.png\nchst35.png  chst41.png  chst47.png  chst53.png  chst59.png  chst65.png\nchst36.png  chst42.png  chst48.png  chst54.png  chst60.png\nchst37.png  chst43.png  chst49.png  chst55.png  chst61.png\nchst38.png  chst44.png  chst50.png  chst56.png  chst62.png\n\n/content/data/val/abd:\nabd0.png  abd1.png  abd2.png  abd3.png  abd4.png\n\n/content/data/val/chest:\nchst0.png  chst1.png  chst2.png  chst3.png  chst4.png\n\n:::\nAs you can see, the data directory contains subdirectories train, val and test, which contain the training, validation and test data for our experiment. train and val contain subdirectories abd and chest containing abdominal and chest radiographs for each data set. There are 65 training images and 10 validation images with balanced distributions over our target classes (i.e.¬†approximately equal numbers of abdominal and chest radiographs in each data set and optimized for a classification problem)."
  },
  {
    "objectID": "posts/Image_Classification_Tutorial.html#model-training-setup",
    "href": "posts/Image_Classification_Tutorial.html#model-training-setup",
    "title": "Image Classification for Beginners",
    "section": "Model Training Setup",
    "text": "Model Training Setup\nBefore we train the model, we have to get the data in a format such that it can be presented to the model for training.\n\nData Loaders\nThe first step is to load the data for the training and validation datasets into a ImageDataLoaders object from the fastai library. When training a model, the ImageDataLoaders will present training - and subsequently, validation - data to the model in batches.\n\n\nData Augmentation\nIn order to be sure that the model isn‚Äôt simply ‚Äúmemorizing‚Äù the training data, we will augment the data by randomly applying different transformations to each image before it is sent to the model.\nTransformations can include rotation, translation, flipping, rescaling, etc.\n\n\nLoad the data into ImageDataLoaders with data augmentation\n\nNote: When you run this next cell in Colab, a batch of data will be shown with or without augmentation transforms applied. (1) Run this cell once with the box next to apply_transforms unchecked to see a sample of the original images. (2) Next, run the cell a few more times after checking the box next to apply_transforms to see what happens to the images when the transforms are applied.\n\n::: {.cell _uuid=‚Äòd1c24e4a78f57f12a42de3481db746fe0f170ee3‚Äô cellView=‚Äòform‚Äô outputId=‚Äòba44aa2d-d8f1-40b2-e9ef-64500483f516‚Äô execution_count=3}\n# the following line of code utilizes Colab Forms\napply_transforms = True #@param {type: 'boolean'}\n\nif apply_transforms:\n    flip = True\n    max_rotate = 10.0\n    max_warp = 0.2\n    p_affine = 0.75\nelse:\n    flip = False\n    max_rotate, max_warp, p_affine = 0, 0, 0\n\ntfms = aug_transforms(\n    do_flip=flip,\n    max_rotate=max_rotate,\n    max_warp=max_warp,\n    p_affine=p_affine,\n    size=224,\n    min_scale=0.75\n)\ndls = ImageDataLoaders.from_folder(path, valid='val', seed=42, item_tfms=Resize(460), batch_tfms=tfms, bs=16)\ndls.show_batch(max_n=6)\n\n\n\n:::\n\n\nFind the optimal learning rate\nThe learning rate is a hyperparameter that controls how much your model adjusts in response to percieved error after each training epoch. Choosing an optimal learning rate is an optimal step in model training.\nFrom the fastai docs: &gt; First introduced by Leslie N. Smith in Cyclical Learning Rates for Training Neural Networks, the LRFinder trains the model with exponentially growing learning rates and stops in case of divergence. &gt; The losses are then plotted against the learning rates with a log scale.  &gt; A good value for the learning rates is then either: &gt; - 1/10th of the minimum before the divergence &gt; - where the slope is the steepest\n\n\nNote: When you run this cell for the first time in a Colab session, it will download a pretrained version of the model to your workspace before running the LRFinder.\n\n\ndls = ImageDataLoaders.from_folder(path, valid='val', seed=42, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75), bs=16)\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.lr_find();\n\nDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth"
  },
  {
    "objectID": "posts/Image_Classification_Tutorial.html#transfer-learning",
    "href": "posts/Image_Classification_Tutorial.html#transfer-learning",
    "title": "Image Classification for Beginners",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nDeep learning requires large amounts of training data to successfully train a model.\nWhen we don‚Äôt have enough data to work with for the planned task, starting with a pre-trained network that has been optimally trained on another task can be helpful. The concept of re-training a pre-trained network for a different task is called transfer learning.\n\nFine-tuning\nIn the process of re-training the model, we start by changing the final layers of the network to define the output or predictions our model will make. In order to avoid propagating too much error through the rest of the network during the initial training, we freeze the other layers of the network for the first cycle or epoch of training. Next, we open up the rest of the network for training and train for a few more epochs. This process is called fine-tuning.\n\n\nEpochs and data augmentation\nDuring each epoch, the model will be exposed to the entire dataset. Each batch of data will have our data transformations randomly applied in order to provide data augmentation. This helps to ensure that our model never sees the exact same image twice. This is important because we wouldn‚Äôt want our model to simply memorize the training dataset and not converge on a generalized solution, resulting in poor performance on the validation dataset.\n\n\nThe loss function\nIn a classification task, you‚Äôre either right or wrong. This binary information doesn‚Äôt give us much nuance to work with when training a model. A loss function give us a numeric estimation of ‚Äúhow wrong‚Äù our model is. This gives us a target to optimize during the training process.\nWhen reviewing the results of successive epochs in training, the loss on your validation dataset should always be decreasing. When it starts to increase, that is a sign of your model overfitting to the training dataset.\n\n\nFine-tuning the model\nWe will fine-tune our model to our task in the following steps: 1. Select the number of epochs for which we will train the model 2. Choose a base learning rate based on the results from the LRFinder plot above 3. Run the cell to initiate model training utilizing the fine_tune() method from fastai\n\nTip: If you‚Äôre running this notebook in Colab, you can re-run this cell with different hyperparameters to better understand how they affect the result.\n\n\n# the following lines of code utilize Colab Forms\nepochs = 5 #@param {type: \"integer\"}\nbase_lr = 2e-3 #@param {type: \"number\"}\n\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.fine_tune(epochs, base_lr=base_lr)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.106555\n1.336308\n0.500000\n00:01\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.124234\n0.632943\n0.700000\n00:01\n\n\n1\n0.175012\n0.107626\n0.900000\n00:01\n\n\n2\n0.129196\n0.022088\n1.000000\n00:01\n\n\n3\n0.100126\n0.012920\n1.000000\n00:01\n\n\n4\n0.078639\n0.012502\n1.000000\n00:01\n\n\n\n\n\n\n\nReview training curves\nThe visual representation of the training and validation losses are useful to evaluate how successfully you were able to train your model. You should see the validation loss continuously decreasing over subsequent batches.\n\nImportant: If the validation loss begins to increase, your model may be starting to overfit. Consider restarting your training experiment with one fewer epochs than it took to overfit.\n\n\nlearn.recorder.plot_loss()"
  },
  {
    "objectID": "posts/Image_Classification_Tutorial.html#testing-the-model",
    "href": "posts/Image_Classification_Tutorial.html#testing-the-model",
    "title": "Image Classification for Beginners",
    "section": "Testing the Model",
    "text": "Testing the Model\n\nTest the model on the test dataset\nWhen you run the following cell, the first line shows the groundtruth for whether the radiograph is of the chest or abdomen. The second line is the model prediction for whether the image is a chest or abdominal radiograph.\n\ntest_files = get_image_files(path/'test')\ntest_dl = learn.dls.test_dl(test_files, with_labels=True)\nlearn.show_results(dl=test_dl)\n\n\n\n\n\n\n\n\n\nA little more detail on the predictions\nRunning this cell will provide us with the loss on each image, as well as the model‚Äôs predicted probability, which can be thought of as the model‚Äôs confidence in its prediction.\n\nNote: If the model is correct and completely confident, the loss should be near ‚Äú0.00‚Äù and the probability will be ‚Äú1.00‚Äù, respectively.\n\n\ninterp = ClassificationInterpretation.from_learner(learn, dl=test_dl)\ninterp.plot_top_losses(k=2)\n\n\n\n\n\n\n\n\n\nTest the model on a surprise example\nHere, we present the model with an unexpected image (an elbow radiograph) and see how it responds.\n\ny = get_image_files(path, recurse=False)\ntest_dl = learn.dls.test_dl(y)\nx, = first(test_dl)\nres = learn.get_preds(dl=test_dl, with_decoded=True)\nx_dec = TensorImage(dls.train.decode((x,))[0][0])\nfig, ax = plt.subplots()\nfig.suptitle('Prediction / Probability', fontsize=14, fontweight='bold')\nx_dec.show(ctx=ax)\nax.set_title(f'{dls.vocab[res[2][0]]} / {max(res[0][0]):.2f}');\n\n\n\n\n\n\n\nWhen presented with this radiograph of an elbow, the model makes a prediction but is less confident than with the other test images.\n\nImportant: (1) A deep learning classification model can only learn what we teach it to learn.\n\n\nImportant: (2) In designing our model implementation, we might consider designing a pre-processing step in which the data (or metadata) is checked to ensure the input to the model is valid."
  },
  {
    "objectID": "posts/Image_Classification_Tutorial.html#visualizing-model-inferences",
    "href": "posts/Image_Classification_Tutorial.html#visualizing-model-inferences",
    "title": "Image Classification for Beginners",
    "section": "Visualizing Model Inferences",
    "text": "Visualizing Model Inferences\n\nClass activation map (CAM)\nCAM allows one to visualize which regions of the original image are heavily weighted in the prediction of the corresponding class. This technique provides a visualization of the activations in the final convolutional block of a Convolutional Neural Network (CNN).\nCAM can also be useful to determine if the model is ‚Äúcheating‚Äù and looking somewhere it shouldn‚Äôt be to make its prediction (i.e.¬†radioopaque markers placed by the technologist).\n\nNote: If you are running this cell in Colab, choose which of the two test images you would like to examine and run this cell to see the CAM output overlayed on the input image.\n\n\ntest_case = 'chest' #@param ['abd', 'chest']\ncls = 0 if test_case == 'abd' else 1\nlabel = test_case\n\ny = get_image_files(path/'test'/label)\ntest_dl = learn.dls.test_dl(y, with_labels=True)\n\nhook = hook_output(learn.model[0])\nx, _ = first(test_dl)\nwith torch.no_grad(): output = learn.model.eval()(x)\nact = hook.stored[0]\ncam_map = torch.einsum('ck,kij-&gt;cij', learn.model[1][-1].weight, act)\nx_dec = TensorImage(dls.train.decode((x,))[0][0])\n_, ax = plt.subplots()\nx_dec.show(ctx=ax)\nax.imshow(cam_map[cls].detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n              interpolation='bilinear', cmap='magma');\nhook.remove()\n\n\n\n\n\n\nGrad-CAM\nGradient-weighted CAM (Grad-CAM) allows us to visualize the output from any convolutional block in a CNN.\nBy default, this cell is setup to show the Grad-CAM output from the final convolutional block in the CNN, for comparison to the CAM output.\n\nNote: If you‚Äôre running this notebook in Colab, (1) choose which of the two test images you would like to examine and run this cell to see the Grad-CAM output overlayed on the input image, then (2) select a different block and re-run the cell to see how the output changes for different blocks in the network.\n\n\ntest_case = 'abd' #@param ['abd', 'chest']\n\ncls = 0 if test_case == 'abd' else 1\nlabel = test_case\n\ny = get_image_files(path/'test'/label)\ntest_dl = learn.dls.test_dl(y, with_labels=True)\nx, _ = first(test_dl)\nmod = learn.model[0]\n\nblock = -2 #@param {type: \"slider\", min: -8, max: -1, step: 1}\n\nhook_func = lambda m,i,o: o[0].detach().clone()\n\nwith Hook(mod[block], hook_func, is_forward=False) as hookg:\n    with Hook(mod[block], hook_func) as hook:\n        output = learn.model.eval()(x.cuda())\n        act = hook.stored\n    output[0, cls].backward()\n    grad = hookg.stored\n\nw = grad[0].mean(dim=[1,2], keepdim=True)\ncam_map = (w * act[0]).sum(0)\nx_dec = TensorImage(dls.train.decode((x,))[0][0])\n_, ax = plt.subplots()\nx_dec.show(ctx=ax)\nax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n              interpolation='bilinear', cmap='magma');"
  },
  {
    "objectID": "posts/llama-cpp-radiology-report-labeling.html",
    "href": "posts/llama-cpp-radiology-report-labeling.html",
    "title": "Radiology Report Labeling with Llama.cpp",
    "section": "",
    "text": "Llama.cpp is a project led by Georgi Gerganov that was initially designed as a pure C/C++ implementation of the Llama large language model developed and open-sourced by Meta‚Äôs AI team.\nQuoted from the llama.cpp GitHub repository:\nIn lay terms, this means that we can implement these models in such a way that they can be run on nearly any physical or virtual machine! You don‚Äôt need an industrial-grade, multi-GPU server to use open-source LLMs locally."
  },
  {
    "objectID": "posts/llama-cpp-radiology-report-labeling.html#when-to-use-an-llm-locally",
    "href": "posts/llama-cpp-radiology-report-labeling.html#when-to-use-an-llm-locally",
    "title": "Radiology Report Labeling with Llama.cpp",
    "section": "When to Use an LLM Locally",
    "text": "When to Use an LLM Locally\n\nYou have sensitive data that you don‚Äôt want to send to OpenAI‚Äôs servers for them to potentially store and use for the training of futures models\n\nVirtually all healthcare data\n\nYou want to fine-tune an open-source LLM for a specific purpose"
  },
  {
    "objectID": "posts/llama-cpp-radiology-report-labeling.html#overview-of-this-module",
    "href": "posts/llama-cpp-radiology-report-labeling.html#overview-of-this-module",
    "title": "Radiology Report Labeling with Llama.cpp",
    "section": "Overview of This Module",
    "text": "Overview of This Module\n\nInstall llama.cpp and Hugging Face Hub (to download model files)\nDownload the 7 billion parameter Llama2 model fine-tuned for chat\nEngineer a prompt to have the LLM read a chest radiography report and return structured labels for specific findings in JSON format.\nTest a few example reports on Llama2-7B-Chat.\nRepeat the process for the Mistral-7B-Instruct-v0.1 model and compare the results.\n\n\nNote: At the time this module was developed, Mistral-7B is the best open-source, 7B parameter model available. This field is moving very quickly, so this very well could change before the end of the year."
  },
  {
    "objectID": "posts/llama-cpp-radiology-report-labeling.html#references",
    "href": "posts/llama-cpp-radiology-report-labeling.html#references",
    "title": "Radiology Report Labeling with Llama.cpp",
    "section": "References",
    "text": "References\n\nLlama.cpp on GitHub: https://github.com/ggerganov/llama.cpp\nMeta AI‚Äôs Llama 2: https://ai.meta.com/llama/\nMistralAI‚Äôs Mistral-7B: https://mistral.ai/news/announcing-mistral-7b/\nHuggingFace Models:\n\nTheBloke/Llama-2-7B-Chat-GGUF\nTheBloke/Mistral-7B-Instruct-v0.1-GGUF\n\n\n\nNote: If you would like to experiment with other models, please search for the ‚ÄúGGUF‚Äù version of the model on Hugging Face.\n\n\n# @title Install llama.cpp and HuggingFace Hub\n# @markdown This cell takes approximately 2 minutes to run. The output is suppressed, so if no error is shown, you may assume that it worked.\n\n%%capture\n\n!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.11 --force-reinstall --upgrade --no-cache-dir\n!pip install huggingface_hub==0.18.0\n\n\n# @title Importing the necessary libraries\n\nfrom huggingface_hub import hf_hub_download\nfrom llama_cpp import Llama\nimport regex as re\nimport json\n\n\n# @title Select the model you'd like to test\n\n# @markdown After initially testing with one model, if you would like to test another then you must change your selection in this cell. Then you will need to re-run this cell and all of the ones below it. You can do this from the `Runtime` menu bar by selecting `Run after`.\n\nmodel = \"llama-2\" # @param [\"llama-2\", \"mistral\"]\n\nif model == \"llama-2\":\n    model_name = \"TheBloke/Llama-2-7b-Chat-GGUF\"\n    model_basename = \"llama-2-7b-chat.Q4_K_M.gguf\"\nelse:\n    model_name = \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\n    model_basename = \"mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n\n\n# @title Download the model from Hugging Face Hub\n\nmodel_path = hf_hub_download(repo_id=model_name, filename=model_basename)\n\n\n\n\n\n# @title Initialize the llama.cpp constructor\n\n# Feel free to play around with different hyperparameters below\n\nlcpp_llm = Llama(\n    model_path=model_path,\n    n_threads=2, # CPU cores\n    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU. Should be a power of 2.\n    n_gpu_layers=36, # Change this value based on your model and your GPU VRAM pool.\n    n_ctx=2048, # Context window = maximum input sequence length (in tokens)\n    n_gqa=8,\n)\n\nAVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |"
  },
  {
    "objectID": "posts/llama-cpp-radiology-report-labeling.html#prompt-engineering",
    "href": "posts/llama-cpp-radiology-report-labeling.html#prompt-engineering",
    "title": "Radiology Report Labeling with Llama.cpp",
    "section": "Prompt Engineering",
    "text": "Prompt Engineering\nPrompt engineering has emerged as an important skill set in getting LLMs to execute your desired task. For this, you should know if there is a prompt template\n\nWe start with a system prompt. This gives the LLM a role to play in the requests that follow.\nWe implement a JSON schema to prompt the LLM to return structured labels for each report we submit.\nWe provide a sample report for the LLM to analyze.\nWe construct the prompt that will present the report text to the model, ask it to use the JSON schema provided, and analyze the report for the findings included in the schema.\nFinally, we utilize the prompt templates for the Llama-2-Chat and Mistral-7B-Instruct-v0.1 models to construct our complete prompt.\n\n\nNote: Mistral-7B does not have a separate delimiter for the system role, so we pass that portion of the prompt with the remainder.\n\nFor more details on prompt engineering, see this guide: Prompt Engineering Guide\n\n# @title System prompt\n\n# @markdown In your experimentation, you may change the text in the following field to see the effect the \"system\" prompt has on the model output.\n\nsystem = \"You are an expert radiologist's assistant, skilled in analyzing radiology reports. Please first provide a response to any specific requests. Then explain your reasoning.\" # @param {type: \"string\"}\n\n\n# @title Construct JSON schema\n\nschema = '''\n{\n    \"cardiomegaly\": { \"type\": \"boolean\" },\n    \"lung_opacity\": { \"type\": \"boolean\" },\n    \"pneumothorax\": { \"type\": \"boolean\" },\n    \"pleural_effusion\": { \"type\": \"boolean\" },\n    \"pulmonary_edema\": { \"type\": \"boolean\" },\n    \"abnormal_study\": { \"type\": \"boolean\" }\n}\n'''\n\n\n# @title Provide a sample chest radiograph report\n\n# @markdown A sample normal chest radiography report is provided for you here. If you would like to experiment, change the text in the field below and re-run this cell and the cells below.\n\nreport_text = \"No focal consolidation, pneumothorax, or pleural effusion. Cardiomediastinal silhouette is stable and unremarkable. No acute osseous abnormalities are identified. No acute cardiopulmonary abnormality.\" # @param {type: \"string\"}\n\n\n# @title Construct User prompt\n\n# @markdown I've included an additional instruction here to help the model understand that there is some overlap between lung opacity and other categories. As you may see below, this can actually confuse some models.\n# @markdown &lt;br&gt;&lt;br&gt;While some prompt engineering techniques can be helpful, you have to experiment to see what produces robust and consistent outputs.\n#@markdown &lt;br&gt;&lt;br&gt;You can delete the following text entirely if you do not want to provide additional instructions.\nadditional_instructions = \"Note that 'lung_opacity' may include nodule, mass, atelectasis, or consolidation.\" # @param {type:\"string\"}\n\nprompt = f'''\n```{report_text}```\nPlease extract the findings from the preceding text radiology report using the following JSON schema:\n```{schema}```\n{additional_instructions}\n'''\n\n\n# @title Llama-2-Chat & Mistral-7B-Instruct-v0.1 prompt templates\n\n# @markdown Using the correct prompt formatting with special tokens like `[INST]` can greatly improve your chances of getting a good response from an LLM. If you're unsure of the appropriate template, check the model card on Hugging Face, or the website or original paper for the model you're using.\n\nllama2_prompt_template = f'''[INST] &lt;&lt;SYS&gt;&gt;\n{system}\n&lt;&lt;/SYS&gt;&gt;\n{prompt}[/INST]\n'''\n\nmistral_prompt_template = f'''&lt;s&gt;[INST] {system} {prompt} [/INST]'''\n\n\n# @title Generate LLM response and print response text\n\nif model == \"llama-2\":\n    full_prompt = llama2_prompt_template\nelse:\n    full_prompt = mistral_prompt_template\n\n#@markdown After initial testing, consider experimenting with some of the hyperparameters below.\n#@markdown - `max_tokens`: maximum model output\n#@markdown - `temperature`: a.k.a. entropy, increases randomness of output. Higher produces more human-like responses. `0` does not guarantee deterministic output.\n#@markdown &lt;p&gt;See the LLM settings guide linked below for more details on experimenting with hyperparameters.\n\nmax_tokens = 512 #@param {type:\"integer\"}\ntemperature = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.1}\ntop_p = 0.95 #@param {type:\"slider\", min:0.8, max:1, step:0.05}\n\nresponse = lcpp_llm(\n    prompt=full_prompt,\n    max_tokens=max_tokens,\n    temperature=temperature,\n    top_p=top_p,\n    repeat_penalty=1.2,\n    top_k=50,\n    # echo=True, # return the prompt\n);\n\nres_txt = response[\"choices\"][0][\"text\"]\nprint(res_txt)\n\nOf course! I'd be happy to help you analyze the radiology report. Here are my findings based on the JSON schema provided:\n{\n\"cardiomegaly\": false,\n\"lung_opacity\": true,\n\"pneumothorax\": false,\n\"pleural_effusion\": false,\n\"pulmonary_rama\": false,\n\"abnormal_study\": true\n}\nExplanation:\nThe report states that there is no focal consolidation, pneumothorax, or pleural effusion. However, it does mention that the cardiomediastinal silhouette is stable and unremarkable, which suggests that there are no signs of cardiac tamponade or other abnormalities in this area. Additionally, the report states that no acute osseous abnormalities were identified, which means that there are no bone fractures or dislocations present. Finally, the report concludes that there is an abnormal study, which indicates that something unusual was detected during the imaging process.\nI hope this helps! Let me know if you have any further questions."
  },
  {
    "objectID": "posts/llama-cpp-radiology-report-labeling.html#limitations-of-this-approach",
    "href": "posts/llama-cpp-radiology-report-labeling.html#limitations-of-this-approach",
    "title": "Radiology Report Labeling with Llama.cpp",
    "section": "Limitations of this Approach",
    "text": "Limitations of this Approach\n\nErrors: You may observe when using Llama-2-7B-Chat that the JSON returned is not ideal for what we requested or may even have an error like turning pulmonary_edema into pulmonary_emia.\n\nThis can be improved by simplifying your request for smaller models or using a model that is better trained for returning structured data in JSON format, like Mistral-7B.\nPlaying around with some of the model inference hyperparameters can also help. See this guide for further details: Prompt Engineering Guide: LLM Settings\n\nHallucinations: LLMs can provide very confident answers that are flat out wrong. You may see output like \"Under the 'lung_opacity' field, the report mentions that there is opacity in both lungs, which could indicate nodules, masses, atelectasis, or consolidation. Therefore, the value for this field is set to true.\", even when there is no mention of that in the report referenced!\n\nThis can be improved by careful prompt engineering. You may want to include in your system prompt an instruction to not return an answer if the model is not confident. Or you may want to try without having the model explain it‚Äôs reasoning.\nA group at NIH found that asking Vicuna-13B to perform a single labeling task at one time provided more robust results in this article published in Radiology: Feasibility of Using the Privacy-preserving Large Language Model Vicuna for Labeling Radiology Reports\nFor certain use cases, retrieval-augmented generation (RAG) can be helpful. We‚Äôll cover that in the next notebook.\nFinally, if all else fails and you have several hundred labeled examples of the task you want the LLM to perform, you may consider parameter-efficient fine-tuning (PEFT). See this guide from NVIDIA for more details: Selecting LLM Customization Techniques\n\n\n\n# @title Define a function to postprocess the response text and extract the JSON object into a Python dict\n\ndef json_from_str(s):\n    expr = re.compile(r'\\{(?:[^{}]*|(?R))*\\}')\n    res = expr.findall(s)\n    return json.loads(res[0]) if res else None\n\n\n# @title Assign an ID number to the report and associate extracted labels with the report ID\n\nid = 1\nlabels = json_from_str(res_txt)\nresult_dict = {id: labels}\nresult_dict\n\n{1: {'cardiomegaly': False,\n  'lung_opacity': True,\n  'pneumothorax': False,\n  'pleural_effusion': False,\n  'pulmonary_rama': False,\n  'abnormal_study': True}}"
  },
  {
    "objectID": "posts/ACR_contrast_manual_chat.html",
    "href": "posts/ACR_contrast_manual_chat.html",
    "title": "Chat with the ACR Contrast Manual",
    "section": "",
    "text": "Retrieval-augmented generation (RAG) is a method of querying existing data to improve the responses of large language models (LLMs) to questions requiring factual and/or specialized knowledge.\nRAG is a two-step process where relevant information is first retrieved from a special database, called a vector database, that stores chunks of text along with the embeddings of that text, typically from a Transformer or LLM.\nEncoding a question or prompt from a user with the same model used to generate the embeddings for the vector DB, one can use similarity search to find relevant chunks of text that can then be presented to an LLM as part of the prompt including the original question.\nIn this way, you can ‚Äúchat‚Äù with any document or database you like. One popular application of this is manual-as-a-service."
  },
  {
    "objectID": "posts/ACR_contrast_manual_chat.html#module-overview",
    "href": "posts/ACR_contrast_manual_chat.html#module-overview",
    "title": "Chat with the ACR Contrast Manual",
    "section": "Module Overview",
    "text": "Module Overview\nIn this module, we will: 1. Split the ACR Contrast Manual into chunks. 2. Create embeddings for the chunks with the MedCPT Article Encoder. 3. Store the chunk:embedding pairs in a vector DB. 4. Set up a RAG Q&A pipeline with the LlamaIndex framework. 5. Test out our RAG Q&A pipeline to ‚Äúchat‚Äù with the ACR Contrast Manual."
  },
  {
    "objectID": "posts/ACR_contrast_manual_chat.html#references",
    "href": "posts/ACR_contrast_manual_chat.html#references",
    "title": "Chat with the ACR Contrast Manual",
    "section": "References",
    "text": "References\n\nhttps://www.promptingguide.ai/techniques/rag\nMedCPT ArXiv paper: https://arxiv.org/abs/2307.00589\n\nMedCPT Article Encoder on Hugging Face: https://huggingface.co/ncbi/MedCPT-Article-Encoder\n\nLlamaIndex LLM Application Framework: https://docs.llamaindex.ai/en/stable/index.html\n\nLlamaIndex llama-cpp-python Integration: https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp.html\n\nLlama2-7B-Chat on Hugging Face: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n\n\n# @title Installing required libraries\n# @markdown This cell will take approximately 4 minutes to run.&lt;br&gt;&lt;br&gt;When the cell finishes running the `Runtime` will be restarted. This will appear as an error saying that your session crashed for an unknown reason.\n# @markdown &lt;br&gt;&lt;br&gt;Don't worry, this is expected. After the error shows up, simply run the next cell to proceed.\n\n%%capture\n!pip uninstall numpy -y\n!pip install numpy==1.25\n!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.11 --force-reinstall --upgrade --no-cache-dir\n!pip install -U \\\n    llama-index==0.8.69.post2 \\\n    huggingface-hub==0.19.3 \\\n    transformers==4.35.2 \\\n    pypdf==3.17.1\n\nimport os\nos.kill(os.getpid(), 9)\n\n\n# @title Import the necessary libraries and functions\nfrom llama_index import VectorStoreIndex, ServiceContext\nfrom llama_index.readers import PDFReader\nfrom llama_index.embeddings import HuggingFaceEmbedding\nfrom llama_index.llms import LlamaCPP\nfrom llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n\nfrom pathlib import Path\n\n\n# @title Download the ACR Contrast Manual\n\n!mkdir pdfs\n!wget -qP /content/pdfs https://www.acr.org/-/media/ACR/Files/Clinical-Resources/Contrast_Media.pdf\n# !wget -qP /content/pdfs https://www.acr.org/-/media/ACR/Files/Radiology-Safety/MR-Safety/Manual-on-MR-Safety.pdf\n\n\n# @title Load the PDF\n\npdf_folder_path = Path(\"/content/pdfs\")\ndocuments = PDFReader().load_data(pdf_folder_path/\"Contrast_Media.pdf\")\n# documents = [PDFReader().load_data(pdf_folder_path/fn) for fn in list(pdf_folder_path.glob('**/*.pdf'))]\n\n\n# @title Obtain our embedding model from Hugging Face Hub\n\nembed_model = HuggingFaceEmbedding(model_name=\"ncbi/MedCPT-Article-Encoder\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# @title We'll be using the Llama-2-7B-Chat model via the Llama.cpp integration\n\nmodel_url = \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf\"\n\nllm = LlamaCPP(\n    # You can pass in the URL to a GGML model to download it automatically\n    model_url=model_url,\n    # optionally, you can set the path to a pre-downloaded model instead of model_url\n    model_path=None,\n    temperature=0.1,\n    max_new_tokens=512,\n    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n    context_window=2048,\n    # kwargs to pass to __call__()\n    generate_kwargs={},\n    # kwargs to pass to __init__()\n    # set to at least 1 to use GPU\n    model_kwargs={\"n_gpu_layers\": 36},\n    # transform inputs into Llama2 format\n    messages_to_prompt=messages_to_prompt,\n    completion_to_prompt=completion_to_prompt,\n    verbose=True\n)\n\nDownloading url https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf to path /tmp/llama_index/models/llama-2-7b-chat.Q4_K_M.gguf\ntotal size (MB): 4081.0\n\n\n3892it [00:29, 130.81it/s]                          \nAVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n\n\n\n# @title Create our `ServiceContext` to specify our custom embeddings and LLM\n# @markdown You can experiment with the `chunk_size` parameter to determine it's effect on inference speed and effective retrieval.\n\nchunk_size = 1000 #@param {type:\"integer\"}\n\nservice_context = ServiceContext.from_defaults(\n    llm=llm,\n    embed_model=embed_model,\n    chunk_size=chunk_size\n)\n\n[nltk_data] Downloading package punkt to /tmp/llama_index...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n\n\n\n# @title Create our `VectorStoreIndex` Query Engine from the PDF and our `ServiceContext`\n# @markdown Another hyperparameter to experiment with is `similarity_top_k`. This is the number of chunks of text that will be retrieved from the `VectorStoreIndex` for each query.\ntop_k = 3 #@param {type:\"integer\"}\n\nindex = VectorStoreIndex.from_documents(\n    documents, service_context=service_context\n)\n\nquery_engine = index.as_query_engine(similarity_top_k=top_k)\n\n\n# @title Test our RAG Q&A pipeline\n\nresponse = query_engine.query(\"What is the GFR threshold below which IV contrast should be withheld in a patient with acute kidney injury?\");\nprint(response)\n\nLlama.generate: prefix-match hit\n\n\n  Thank you for providing additional context. Based on the updated information, there is no specific GFR threshold mentioned in the provided references that indicates when to withhold IV contrast in patients with acute kidney injury. However, it is suggested that if a threshold for CI-AKI risk is used at all, 30 mL/min/1.73m2 seems to be the one with the greatest level of evidence [96].\nIt is important to note that no serum creatinine or eGFR threshold is adequate to stratify risk for patients with AKI because serum creatinine in this setting is unreliable [134, 135]. Therefore, any threshold used must be weighed on an individual patient level with the benefits of administering contrast material.\nIn summary, while there is no specific GFR threshold mentioned in the provided references for withholding IV contrast in patients with acute kidney injury, a threshold of 30 mL/min/1.73m2 seems to be the most commonly cited and evidence-based recommendation. However, it is important to consider individual patient factors and the benefits of contrast material administration when making decisions regarding contrast use in patients with AKI.\n\n\nPlay around with different queries and see how well the model responds.\nYou could even try different embedding models and LLMs available on Hugging Face.\n\n#@title Test other questions\n#@markdown Edit the text below and re-run the cell to try a different question.\n\nquestion = \"What considerations should I be aware of when giving IV contrast to a patient taking metformin?\" #@param {type:\"string\"}\nresponse = query_engine.query(question);\nprint(response)\n\nLlama.generate: prefix-match hit\nLlama.generate: prefix-match hit\n\n\n  Thank you for providing additional context. Based on the new information provided, here is a refined answer to your original question:\nWhen giving IV contrast to a patient taking metformin, it is essential to consider several factors to ensure safe and effective treatment. As an honest and respectful assistant, I must inform you that I cannot provide medical advice or make recommendations without proper training and qualifications. However, I can provide general information on the considerations that should be taken into account when administering IV contrast to a patient taking metformin.\nFirstly, it is important to consult with a qualified medical professional who can assess the patient's individual risks and benefits of administering IV contrast. This includes discussing potential risks such as contrast-induced nephropathy, which may be more likely in patients with chronic kidney disease or those taking medications that affect kidney function, such as metformin.\nSecondly, it is important to ensure that the patient is properly hydrated before and after administering IV contrast. This can help reduce the risk of contrast-induced nephropathy by minimizing the amount of contrast material that reaches the kidneys.\nThirdly, it may be beneficial to consider alternative imaging modalities that do not require the use of IV contrast, such as ultrasound or magnetic resonance imaging (MRI). These alternatives may be more appropriate for patients with renal impairment or those taking medications that affect kidney function.\nFinally, it is important to monitor the patient's kidney function closely after administering IV contrast, particularly in patients with pre-existing renal disease or those taking medications that affect kidney function. This may involve regular monitoring of serum creatinine levels and other markers of kidney function.\nIn summary, when giving IV contrast to a patient taking metformin, it is essential to consult with a qualified medical professional who can assess the patient's individual risks and benefits of administering IV contrast. Proper hydration, consideration of alternative imaging modalities, and close monitoring of kidney function are also important factors to consider when administering IV contrast to patients with renal impairment or those taking medications that affect kidney function.\nI hope this refined answer is helpful in addressing your query. If you have any further questions or concerns, please do not hesitate to"
  },
  {
    "objectID": "posts/ACR_contrast_manual_chat.html#inspecting-the-retrieved-source-texts",
    "href": "posts/ACR_contrast_manual_chat.html#inspecting-the-retrieved-source-texts",
    "title": "Chat with the ACR Contrast Manual",
    "section": "Inspecting the Retrieved Source Texts",
    "text": "Inspecting the Retrieved Source Texts\nSometimes, it can be helpful to see what source texts were provided as context for the LLM to answer your query. This can help in troubleshooting and determining whether to increase the number of texts retrieved for each query.\n\nresponse = query_engine.query(\"Should metformin be discontinued when giving IV gadolinium-based contrast?\");\nprint(response)\nsources = response.get_formatted_sources(length=1000)\nprint(sources)\n\nLlama.generate: prefix-match hit\n\n\n  Based on the provided context information, there is no clear indication to discontinue metformin before administering IV gadolinium-based contrast. The American College of Radiology (ACR) Manual on Contrast Media states that \"there is no evidence to suggest that metformin should be discontined prior to administration of gadolinium\" (p. 53). In fact, the ACR recommends that patients with chronic kidney disease (CKD) who are receiving metformin and require IV contrast should continue their medication unless there are contraindications or significant interactions (ACR Manual on Contrast Media, p. 53).\nIt is important to note that the risk of contrast-induced nephropathy (CIN) in patients with CKD is highly dependent on the degree of kidney dysfunction and the amount of contrast used. Therefore, it is crucial to carefully evaluate each patient's individual risks and benefits before administering IV contrast.\nIn summary, based on the provided context information, there is no clear indication to discontinue metformin before administering IV gadolinium-based contrast. Patients with CKD who are receiving metformin should continue their medication unless there are contraindications or significant interactions, and their individual risks and benefits should be carefully evaluated before administering IV contrast.\n&gt; Source (Doc id: 063f642c-45d8-45d5-a037-a81418c976c0): Preface¬†\n2¬†\n \n \n  \n \n  \n \n \n \nACR Manual \nOn \nContrast Media  \n \n \n2023 \n \n \n \nACR Committee  \non   \n                Drugs and  Contrast Media\n\n&gt; Source (Doc id: 904d37ad-2103-4d5c-a3b8-592dee3932c1): ACR Manual on Contrast \nMedia  \n \n \n2023  \n \nACR Committee on \nDrugs  and Contrast \nMedia  \n \n \n  \n \n \n \n  \n \n \n \n \n    \n \n \n¬© Copyright 2023 American College of Radiology ISBN: 978-1-55903-012-0\n\n&gt; Source (Doc id: 5ecab8fe-8373-4f36-9bbf-554371400d4e): CONTRAST-ASSOCIATED ACUTE KIDNEY INJURY AND CONTRAST-INDUCED ACUTE KIDNEY INJURY IN ADULTS  50 ¬†90.¬† Merten¬†GJ,¬†Burgess¬†WP,¬†Gray¬†LV,¬†et¬†al.¬†Prevention ¬†of¬†contrast‚Äêinduced¬†nephropathy ¬†with¬†sodium¬†bicarbonate: ¬†a¬†randomized ¬†\ncontrolled ¬†trial.¬†Jama¬†2004;291:2328 ‚Äê34.¬†\n91.¬† Navaneethan ¬†SD,¬†Singh¬†S,¬†Appasamy ¬†S,¬†Wing¬†RE,¬†Sehgal¬†AR.¬†Sodium¬†bicarbonate ¬†therapy¬†for¬†prevention ¬†of¬†contrast‚Äêinduced¬†\nnephropathy: ¬†a¬†systematic ¬†review¬†and¬†meta‚Äêanalysis.¬†American ¬†journal¬†of¬†kidney¬†diseases¬†:¬†the¬†official¬†journal¬†of¬†the¬†National¬†\nKidney¬†Foundation ¬†2009;53:617 ‚Äê27.¬†\n92.¬† Taylor¬†AJ,¬†Hotchkiss ¬†D,¬†Morse¬†RW,¬†McCabe¬†J.¬†PREPARED: ¬†Preparation ¬†for¬†Angiography ¬†in¬†Renal¬†Dysfunction: ¬†a¬†randomized ¬†trial¬†of¬†\ninpatient¬†vs¬†outpatient ¬†hydration ¬†protocols ¬†for¬†cardiac¬†catheterization ¬†in¬†mild‚Äêto‚Äêmoderate ¬†renal¬†dysfunction. ¬†Chest¬†\n1998;114:1570 ‚Äê4.¬†\n93.¬† Zoungas¬†S,¬†Ninomiya ¬†T,¬†Huxley¬†R,¬†et¬†al.¬†Systematic ¬†review:¬†sodium¬†bicarbonate ¬†treatment ¬†regimens ¬†for¬†the¬†prevention ¬†of¬†\ncontrast‚Äêinduced¬†n...\n\n\n\nTroubleshooting RAG\nSome of the issues that can arise with the approach implemented above include: - Sources with a lot of white space - Header/footer material that isn‚Äôt very useful - Irrelevant sources\nRetrieval via nearest neighbor approaches with embeddings are imperfect. Some additional techniques you can try to make your RAG approach more robust include: - Data cleaning prior to embedding and input into the vector database - Re-ranking after retrieval - Sentence-window retrieval - Auto-merging retrieval\nTo learn more about these methods, see the following free course from DeepLearning.ai: https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/"
  },
  {
    "objectID": "posts/ACR_contrast_manual_chat.html#more-test-queries",
    "href": "posts/ACR_contrast_manual_chat.html#more-test-queries",
    "title": "Chat with the ACR Contrast Manual",
    "section": "More Test Queries",
    "text": "More Test Queries\n\nresponse = query_engine.query(\"What are some of the concerns regarding children and IV contrast?\");\nprint(response)\n\nLlama.generate: prefix-match hit\n\n\n  Based on the provided context, here are some concerns regarding children and IV contrast:\n1. Acute reactions to contrast media in children can be severe and require immediate medical attention.\n2. Children are at a higher risk for extravasation-related complications due to their smaller body size and immature circulatory system.\n3. The signs and symptoms of extravasation can be subtle and may not always be obvious, making it crucial to closely monitor children during and after contrast media injection.\n4. Children with underlying medical conditions or those who are severely ill or debilitated are at a higher risk for complications from contrast media extravasation.\n5. The choice of injection site can affect the risk of extravasation, and certain sites (e.g., hand, wrist, foot, and ankle) may be more prone to complications.\n6. Injection rates may also play a role in the risk of extravasation, with higher flow rates potentially increasing the likelihood of complications.\n7. Children who are unable to communicate effectively (e.g., infants, young children, and elderly patients) may be at a higher risk for extravasation due to their limited ability to express any discomfort or pain.\n8. Women may also have a mild increased risk of extravasation compared to men.\n9. Some conditions, such as altered circulation or prior radiation therapy, can increase the risk of complications from contrast media extravasation in children.\n10. It is essential to follow proper injection techniques and guidelines to minimize the risk of complications in children undergoing IV contrast media injection.\n\n\n\nresponse = query_engine.query(\"When should surgical consultation be obtained after contrast extravasation?\");\nprint(response)\n\nLlama.generate: prefix-match hit\nLlama.generate: prefix-match hit\n\n\n  Thank you for providing additional context. Based on the updated information, when should surgical consultation be obtained after contrast extravasation?\nSurgical consultation should be obtained urgently when there is concern for a severe extravasation injury, such as:\n* Severe pain that persists or worsens over time\n* Progressive swelling or pain that cannot be controlled with elevation or other measures\n* Altered tissue perfusion as evidenced by decreased capillary refill, which can indicate ischemia or necrosis\n* Change in sensation in the affected limb, such as numbness or tingling\n* Worsening passive or active range of motion, which can indicate muscle or nerve damage\n* Skin ulceration or blistering, which can indicate infection or necrosis\n\nIt is important to closely monitor the patient and seek surgical consultation if any of these signs or symptoms develop. While some interventions such as warm or cold compresses may be helpful in managing symptoms, there is no clear evidence favoring one over the other, and aspiration of the extravasated contrast medium is not recommended. Topical application of silver sulfadiazine ointment and steroid cream may be useful in soothing irritated skin and preventing infection, but its efficacy is unknown. Hyaluronidase has been used in the management of unrelated medication extravasations, but there is no adequate evidence to support its use after contrast material extravasation.\nIn summary, surgical consultation should be obtained urgently when there are signs of a severe extravasation injury, and close monitoring and prompt intervention are crucial to prevent long-term complications.\n\n\n\nresponse = query_engine.query(\"Can I power-inject contrast into a PICC line?\");\nprint(response)\n\nLlama.generate: prefix-match hit\nLlama.generate: prefix-match hit\n\n\n  Thank you for providing additional context. Based on the updated information, the answer to the query remains the same as the original answer. Power-injecting contrast into a PICC line is not recommended due to the risk of tip migrations and other complications. It is important to follow manufacturer recommendations and only use power injection through certified port sites. Mechanical injections can be performed through some pressure-injectable peripherally inserted central catheters (PICCs), but it is crucial to ensure that the port site is certified as power-injectable before using a central venous line for power injection.\nTherefore, the answer to the query remains: No, you should not power-inject contrast into a PICC line.\n\n\n\nresponse = query_engine.query(\"When should kidney function be checked prior to giving iodinated contrast?\");\nprint(response)\n\nLlama.generate: prefix-match hit\nLlama.generate: prefix-match hit\n\n\n  Thank you for providing additional context! Based on the new information provided, the recommended interval for checking kidney function prior to giving iodinated contrast may vary depending on the individual patient's risk factors and medical history. In general, it is suggested to check kidney function in patients who have a new risk factor or heightened risk of renal dysfunction, such as those with a history of kidney disease, diabetes, or heart failure, within 30 days to 1 week prior to giving iodinated contrast. However, for patients who are taking medications that can affect kidney function, such as non-steroidal anti-inflammatory drugs (NSAIDs) or corticosteroids, it is recommended to check kidney function more frequently, ideally within 24 hours of administering these medications. Additionally, patients who have a history of contrast-induced nephropathy or acute kidney injury may require more frequent monitoring of their kidney function, typically every 1-2 days for the first week after administration of iodinated contrast. It is important to consult with a healthcare professional for specific guidance on when to check kidney function prior to administering iodinated contrast based on individual patient factors.\n\n\n\nresponse = query_engine.query(\"In what situations should kidney function be checked prior to iodinated contrast?\");\nprint(response)\n\nLlama.generate: prefix-match hit\nLlama.generate: prefix-match hit\n\n\n  Thank you for providing additional context! Based on the updated information, it is important to check kidney function before administering iodinated contrast in situations where there are new risk factors or a new risk of renal dysfunction, such as inpatients or those with pre-existing kidney disease. It is also recommended to assess kidney function in patients taking medications that can affect the kidneys, such as non-steroidal anti-inflammatory drugs (NSAIDs).\nIn summary, the refined answer is:\nKidney function should be checked prior to administering iodinated contrast in the following situations:\n1. New risk factors or a new risk of renal dysfunction: If a patient has recently developed new risk factors for contrast-induced nephropathy, such as diabetes, hypertension, or a history of kidney disease, it may be prudent to assess their kidney function before administering iodinated contrast.\n2. Inpatients: Inpatients are at a higher risk for contrast-induced nephropathy compared to outpatients. Therefore, it is recommended to check kidney function before administering iodinated contrast in this population.\n3. Those with a heightened risk of renal dysfunction: Patients with pre-existing kidney disease or those taking medications that can affect the kidneys, such as NSAIDs, should have their kidney function checked before administering iodinated contrast.\nIt is important to note that these are general guidelines and the specific risk factors and assessment protocols may vary depending on individual patient circumstances and medical center policies. It is always best to consult with a qualified healthcare professional for personalized advice."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Radiology AI Tutorials",
    "section": "",
    "text": "Welcome!\nEach of the posts below is an interactive tutorial. For the best experience, you should have a Google account, so you can open the notebooks in Google Colab and run/interact with the demonstrations.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nRadiology Report Labeling with Llama.cpp\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nChat with the ACR Contrast Manual\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nImage Classification for Beginners\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  }
]