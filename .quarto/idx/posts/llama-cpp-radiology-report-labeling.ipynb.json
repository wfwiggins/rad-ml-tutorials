{"title":"Radiology Report Labeling with Llama.cpp","markdown":{"headingText":"jupyter: python3","containsRefs":false,"markdown":"\n<a href=\"https://colab.research.google.com/github/wfwiggins/rad-ml-tutor/blob/master/_notebooks/2023-12-11-llama-cpp-radiology-report-labeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n---\ntitle: \"Radiology Report Labeling with Llama.cpp\"\nauthor: \"Walter Wiggins\"\ndate: \"2023-12-12\"\ncategories: [code, LLM]\nformat:\n    html:\n        code-fold: false\n\n> **_Feel free to save a copy on your Google Drive before you begin._**\n\nLlama.cpp is a project led by Georgi Gerganov that was initially designed as a pure C/C++ implementation of the Llama large language model developed and open-sourced by Meta's AI team.\n\nQuoted from the llama.cpp GitHub repository:\n\n>The main goal of llama.cpp is to run the LLaMA model using 4-bit integer quantization on a MacBook\n> - Plain C/C++ implementation without dependencies\n> - Apple silicon first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks\n> - AVX, AVX2 and AVX512 support for x86 architectures\n> - Mixed F16 / F32 precision\n> - 2-bit, 3-bit, 4-bit, 5-bit, 6-bit and 8-bit integer quantization support\n> - CUDA, Metal and OpenCL GPU backend support\n\nIn lay terms, this means that we can implement these models in such a way that they can be run on nearly any physical or virtual machine! **You don't need an industrial-grade, multi-GPU server to use open-source LLMs locally.**\n\n## When to Use an LLM Locally\n* You have sensitive data that you don't want to send to OpenAI's servers for them to potentially store and use for the training of futures models\n    - Virtually all healthcare data\n* You want to fine-tune an open-source LLM for a specific purpose\n\n## Overview of This Module\n1. Install llama.cpp and Hugging Face Hub (to download model files)\n2. Download the 7 billion parameter Llama2 model fine-tuned for chat\n3. Engineer a prompt to have the LLM read a chest radiography report and return structured labels for specific findings in JSON format.\n4. Test a few example reports on Llama2-7B-Chat.\n5. Repeat the process for the Mistral-7B-Instruct-v0.1 model and compare the results.\n\n> _Note: At the time this module was developed, Mistral-7B is the best open-source, 7B parameter model available. This field is moving very quickly, so this very well could change before the end of the year._\n\n## References\n- Llama.cpp on GitHub: https://github.com/ggerganov/llama.cpp\n- Meta AI's Llama 2: https://ai.meta.com/llama/\n- MistralAI's Mistral-7B: https://mistral.ai/news/announcing-mistral-7b/\n- HuggingFace Models:\n    * [TheBloke/Llama-2-7B-Chat-GGUF](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF)\n    * [TheBloke/Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF)\n\n> _Note: If you would like to experiment with other models, please search for the \"GGUF\" version of the model on Hugging Face._\n\n## Prompt Engineering\n\nPrompt engineering has emerged as an important skill set in getting LLMs to execute your desired task. For this, you should know if there is a **prompt template**\n\n1. We start with a `system` prompt. This gives the LLM a role to play in the requests that follow.\n2. We implement a JSON `schema` to prompt the LLM to return structured labels for each report we submit.\n3. We provide a sample `report` for the LLM to analyze.\n4. We construct the `prompt` that will present the report text to the model, ask it to use the JSON schema provided, and analyze the report for the findings included in the schema.\n5. Finally, we utilize the `prompt templates` for the Llama-2-Chat and Mistral-7B-Instruct-v0.1 models to construct our complete prompt.\n\n> _Note: Mistral-7B does not have a separate delimiter for the system role, so we pass that portion of the prompt with the remainder._\n\nFor more details on prompt engineering, see this guide: [Prompt Engineering Guide](https://www.promptingguide.ai/)\n\n## Limitations of this Approach\n\n1. **Errors:** You may observe when using Llama-2-7B-Chat that the JSON returned is not ideal for what we requested or may even have an error like turning `pulmonary_edema` into `pulmonary_emia`.\n    - This can be improved by simplifying your request for smaller models or using a model that is better trained for returning structured data in JSON format, like Mistral-7B.\n    - Playing around with some of the model inference hyperparameters can also help. See this guide for further details: [Prompt Engineering Guide: LLM Settings](https://www.promptingguide.ai/introduction/settings)\n2. **Hallucinations:** LLMs can provide very confident answers that are flat out wrong. You may see output like `\"Under the 'lung_opacity' field, the report mentions that there is opacity in both lungs, which could indicate nodules, masses, atelectasis, or consolidation. Therefore, the value for this field is set to true.\"`, even when there is no mention of that in the report referenced!\n    - This can be improved by careful prompt engineering. You may want to include in your `system prompt` an instruction to not return an answer if the model is not confident. Or you may want to try without having the model explain it's reasoning.\n    - A group at NIH found that asking Vicuna-13B to perform a single labeling task at one time provided more robust results in this article published in _Radiology_: [Feasibility of Using the Privacy-preserving Large Language Model Vicuna for Labeling Radiology Reports](https://pubs.rsna.org/doi/10.1148/radiol.231147)\n    - For certain use cases, retrieval-augmented generation (RAG) can be helpful. We'll cover that in the next notebook.\n    - Finally, if all else fails and you have several hundred labeled examples of the task you want the LLM to perform, you may consider parameter-efficient fine-tuning (PEFT). See this guide from NVIDIA for more details: [Selecting LLM Customization Techniques](https://developer.nvidia.com/blog/selecting-large-language-model-customization-techniques/)\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"llama-cpp-radiology-report-labeling.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"cosmo","title-block-banner":true,"title":"Radiology Report Labeling with Llama.cpp","author":"Walter Wiggins","date":"2023-12-12","categories":["code","LLM"],"jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}