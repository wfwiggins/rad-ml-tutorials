{"title":"Image Classification for Beginners","markdown":{"headingText":"jupyter: python3","containsRefs":false,"markdown":"\n<a href=\"https://colab.research.google.com/github/wfwiggins/rad-ml-tutor/blob/master/Image_Classification_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n---\ntitle: \"Image Classification for Beginners\"\nauthor: \"Walter Wiggins\"\ndate: \"2023-11-30\"\ncategories: [code, imaging, classification]\nformat:\n    html:\n        code-fold: false\n\n![RSNA logo](https://github.com/wfwiggins/RSNA-Image-AI-2020/blob/master/images/RSNA-2020-logo-with-dates.gif?raw=true)\n\n## Introduction\nIn this demonstration, we will utilize techniques of _computer vision_, including deep _convolutional neural networks_ (CNNs), to train an image classifier model capable of classifying radiographs as either **chest** or **abdominal**.\n\n### Code\nWe will utilize the [fast.ai v2 library](https://docs.fast.ai/), written primarily by Jeremy Howard and Sylvain Gugger (with help from many others). It is written in the [Python programming language](https://www.python.org/) and built on top of the [PyTorch deep learning library](https://www.pytorch.org/).\n\nThe demonstration in this notebook relies heavily on examples from the `fast.ai` book, _Deep Learning for Coders with fastai and PyTorch: AI Applications without a PhD_ by Jeremy Howard and Sylvain Gugger, which was written entirely in Jupyter notebooks, which are [freely available for download on GitHub](https://github.com/fastai/fastbook). A print copy of the book can be purchased from Amazon.\n\n### Data\nThis work is adapted from \"[Hello World Deep Learning in Medical Imaging](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5959832/) {% fn 1 %}\". The chest and abdominal radiographs were obtained from [Paras Lakhani's GitHub repository](https://github.com/paras42/Hello_World_Deep_Learning/tree/9921a12c905c00a88898121d5dc538e3b524e520).\n\n{{ \"_Reference:_ Lakhani P, Gray DL, Pett CR, Nagy P, Shih G. Hello World Deep Learning in Medical Imaging. J Digit Imaging. 2018 Jun; 31(3):283-289. Published online 2018 May 3. doi: 10.1007/s10278-018-0779-6\" | fndetail: 1 }}\n\n### Developers\n- Walter F. Wiggins, MD, PhD - Duke University Hospital, Durham, NC, USA\n- Kirti Magudia, MD, PhD, - University of California, San Francisco, CA, USA\n- M. Travis Caton, MD, PhD - University of California, San Francisco, CA, USA\n\n### Acknowledgements\nOther versions of this notebook implemented on the [Kaggle Notebooks platform](https://www.kaggle.com/wfwiggins203/hello-world-for-deep-learning-siim) were presented at the 2019 Society for Imaging Informatics in Medicine (SIIM) Annual Meeting and for the American College of Radiology (ACR) Residents & Fellows Section (RFS) [AI Journal Club](https://www.acr.org/Member-Resources/rfs/Journal-Club).\n\nWe would also like to acknowledge the following individuals for inspiring our transition to the Google Colab platform with their excellent notebook from the 2019 RSNA AI Refresher Course:\n- Luciano M. Prevedello, MD, PhD\n- Felipe C. Kitamura, MD, MSc\n- Igor Santos, MD\n- Ian Pan, MD\n\n\n## System Setup & Downloading the Data\n\n> Important: Save a copy of this notebook in your Google Drive folder by selecting _Save a Copy in Drive_ from the _File_ menu in the top left corner of this page. This will allow you to modify the cells and save your results.\n\n<br>\n\n> Warning: Make sure you have the _runtime type_ set to **\"GPU\"**. See GIF below.\n\n<br>\n\n![Set Runtime to GPU](https://github.com/wfwiggins/RSNA-Image-AI-2020/blob/master/images/set-runtime-type.gif?raw=true)\n\n\n### Setting up the runtime environment...\nRunning the following cell in Colab will install the necessary libraries, download the data and restart the session.\n\n> Warning: This will generate an error message, which we can safely ignore ðŸ˜‰.\n\n## Exploring the Data\nLet's take a look at the directory structure and contents, then create some variables to help us as we proceed.\n\nAs you can see, the `data` directory contains subdirectories `train`, `val` and `test`, which contain the *training*, *validation* and *test* data for our experiment. `train` and `val` contain subdirectories `abd` and `chest` containing abdominal and chest radiographs for each data set. There are 65 training images and 10 validation images with *balanced distributions* over our *target classes* (i.e. approximately equal numbers of abdominal and chest radiographs in each data set and optimized for a classification problem). \n\n## Model Training Setup\n\nBefore we train the model, we have to get the data in a format such that it can be presented to the model for training.\n\n### Data Loaders\nThe first step is to load the data for the training and validation datasets into a `ImageDataLoaders` object from the `fastai` library. When training a model, the `ImageDataLoaders` will present training - and subsequently, validation - data to the model in _batches_.\n\n### Data Augmentation\nIn order to be sure that the model isn't simply \"memorizing\" the training data, we will _augment_ the data by randomly applying different _transformations_ to each image before it is sent to the model.\n\nTransformations can include rotation, translation, flipping, rescaling, etc.\n\n### Load the data into `ImageDataLoaders` with data augmentation\n\n> Note: When you run this next cell in Colab, a batch of data will be shown with or without augmentation transforms applied. (1) Run this cell once with the box next to `apply_transforms` unchecked to see a sample of the original images. (2) Next, run the cell a few more times after checking the box next to `apply_transforms` to see what happens to the images when the transforms are applied.\n\n### Find the optimal learning rate\nThe learning rate is a hyperparameter that controls how much your model adjusts in response to percieved error after each training epoch. Choosing an optimal learning rate is an optimal step in model training.\n\nFrom the `fastai` [docs](https://docs.fast.ai/callback.schedule#Learner.lr_find):\n> First introduced by Leslie N. Smith in [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/pdf/1506.01186.pdf), the `LRFinder` trains the model with exponentially growing learning rates and stops in case of divergence.\n> The losses are then plotted against the learning rates with a log scale. <br><br>\n> A good value for the learning rates is then either:\n> - 1/10th of the minimum before the divergence\n> - where the slope is the steepest\n\n<br>\n\n> Note: When you run this cell for the first time in a Colab session, it will download a pretrained version of the model to your workspace before running the `LRFinder`.\n\n## Transfer Learning\n\nDeep learning requires large amounts of training data to successfully train a model.\n\nWhen we don't have enough data to work with for the planned task, starting with a _pre-trained_ network that has been optimally trained on another task can be helpful. The concept of re-training a pre-trained network for a different task is called _transfer learning_.\n\n### Fine-tuning\n\nIn the process of re-training the model, we start by changing the final layers of the network to define the output or predictions our model will make. In order to avoid propagating too much error through the rest of the network during the initial training, we freeze the other layers of the network for the first cycle or _epoch_ of training. Next, we open up the rest of the network for training and train for a few more _epochs_. This process is called _fine-tuning_.\n\n### Epochs and data augmentation\n\nDuring each epoch, the model will be exposed to the entire dataset. Each batch of data will have our data transformations randomly applied in order to provide data augmentation. This helps to ensure that our model never sees the exact same image twice. This is important because we wouldn't want our model to simply memorize the training dataset and not converge on a generalized solution, resulting in poor performance on the validation dataset.\n\n### The loss function\n\nIn a classification task, you're either right or wrong. This binary information doesn't give us much nuance to work with when training a model. A _loss function_ give us a numeric estimation of \"how wrong\" our model is. This gives us a target to optimize during the training process.\n\nWhen reviewing the results of successive epochs in training, the loss on your validation dataset should always be **decreasing**. When it starts to increase, that is a sign of your model _overfitting_ to the training dataset.\n\n### Fine-tuning the model\nWe will fine-tune our model to our task in the following steps:\n1. Select the number of epochs for which we will train the model\n2. Choose a base learning rate based on the results from the `LRFinder` plot above\n3. Run the cell to initiate model training utilizing the `fine_tune()` method from `fastai`\n\n> Tip: If you're running this notebook in Colab, you can re-run this cell with different hyperparameters to better understand how they affect the result.\n\n### Review training curves\nThe visual representation of the training and validation losses are useful to evaluate how successfully you were able to train your model. You should see the validation loss continuously decreasing over subsequent batches.\n\n> Important: If the validation loss begins to increase, your model may be starting to **overfit**. Consider restarting your training experiment with one fewer epochs than it took to overfit. \n\n## Testing the Model\n\n### Test the model on the test dataset\nWhen you run the following cell, the first line shows the groundtruth for whether the radiograph is of the chest or abdomen. The second line is the model prediction for whether the image is a chest or abdominal radiograph.\n\n### A little more detail on the predictions\nRunning this cell will provide us with the loss on each image, as well as the model's predicted probability, which can be thought of as the model's confidence in its prediction.\n\n> Note: If the model is correct and completely confident, the loss should be near \"0.00\" and the probability will be \"1.00\", respectively.\n\n### Test the model on a surprise example\nHere, we present the model with an unexpected image (an elbow radiograph) and see how it responds. \n\nWhen presented with this radiograph of an elbow, the model makes a prediction but is less confident than with the other test images.\n\n> Important: (1) A deep learning classification model _can only learn what we teach it to learn_.\n\n> Important: (2) In designing our model implementation, we might consider designing a pre-processing step in which the data (or metadata) is checked to ensure the input to the model is valid.\n\n## Visualizing Model Inferences\n\n### Class activation map (CAM)\n\nCAM allows one to visualize which regions of the original image are heavily weighted in the prediction of the corresponding class. This technique provides a visualization of the activations in the **final** _convolutional_ block of a Convolutional Neural Network (CNN).\n\nCAM can also be useful to determine if the model is \"cheating\" and looking somewhere it shouldn't be to make its prediction (i.e. radioopaque markers placed by the technologist).\n\n> Note: If you are running this cell in Colab, choose which of the two test images you would like to examine and run this cell to see the CAM output overlayed on the input image.\n\n### Grad-CAM\nGradient-weighted CAM (Grad-CAM) allows us to visualize the output from _any convolutional block_ in a CNN.\n\nBy default, this cell is setup to show the Grad-CAM output from the final convolutional block in the CNN, for comparison to the CAM output.\n\n> Note: If you're running this notebook in Colab, (1) choose which of the two test images you would like to examine and run this cell to see the Grad-CAM output overlayed on the input image, then (2) select a _different_ block and re-run the cell to see how the output changes for different blocks in the network.\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"Image_Classification_Tutorial.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"cosmo","title-block-banner":true,"title":"Image Classification for Beginners","author":"Walter Wiggins","date":"2023-11-30","categories":["code","imaging","classification"],"jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}